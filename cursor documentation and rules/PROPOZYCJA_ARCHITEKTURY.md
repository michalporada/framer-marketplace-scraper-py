# Propozycja Architektury Projektu - Scraper Framer Marketplace

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ scrape.yml              # GitHub Actions - scheduled scraping
â”‚       â”œâ”€â”€ ci.yml                   # GitHub Actions - CI/CD
â”‚       â””â”€â”€ backup.yml               # GitHub Actions - backup danych
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ marketplace_scraper.py  # GÅ‚Ã³wny scraper marketplace
â”‚   â”‚   â”œâ”€â”€ product_scraper.py       # Scraper pojedynczego produktu
â”‚   â”‚   â”œâ”€â”€ creator_scraper.py       # Scraper profilu twÃ³rcy
â”‚   â”‚   â”œâ”€â”€ category_scraper.py      # Scraper kategorii
â”‚   â”‚   â””â”€â”€ sitemap_scraper.py       # Scraper sitemap.xml
â”‚   â”‚
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ product_parser.py       # Parsowanie danych produktu
â”‚   â”‚   â”œâ”€â”€ creator_parser.py       # Parsowanie danych twÃ³rcy
â”‚   â”‚   â””â”€â”€ category_parser.py       # Parsowanie danych kategorii
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ product.py              # Model Pydantic produktu
â”‚   â”‚   â”œâ”€â”€ creator.py              # Model Pydantic twÃ³rcy
â”‚   â”‚   â””â”€â”€ category.py             # Model Pydantic kategorii
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ file_storage.py         # Zapis do plikÃ³w (JSON, CSV)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py        # Ograniczenie czÄ™stotliwoÅ›ci requestÃ³w
â”‚   â”‚   â”œâ”€â”€ user_agents.py          # Rotacja User-Agent
â”‚   â”‚   â”œâ”€â”€ logger.py               # Konfiguracja logowania
â”‚   â”‚   â”œâ”€â”€ retry.py                # Retry logic
â”‚   â”‚   â”œâ”€â”€ normalizers.py          # Normalizacja dat i statystyk (Opcja B) â­
â”‚   â”‚   â”œâ”€â”€ checkpoint.py           # Checkpoint system (resume capability)
â”‚   â”‚   â””â”€â”€ metrics.py              # Tracking metryk scrapowania
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py             # Konfiguracja (pydantic-settings)
â”‚   â”‚
â”‚   â””â”€â”€ main.py                     # Entry point aplikacji
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/                   # Zapisane dane produktÃ³w (JSON)
â”‚   â”‚   â”œâ”€â”€ templates/              # Szablony ({product_id}.json)
â”‚   â”‚   â”œâ”€â”€ components/             # Komponenty ({product_id}.json)
â”‚   â”‚   â”œâ”€â”€ vectors/                # Wektory ({product_id}.json)
â”‚   â”‚   â””â”€â”€ plugins/                 # Wtyczki ({product_id}.json) â­
â”‚   â”œâ”€â”€ creators/                   # Profile twÃ³rcÃ³w jako osobne pliki JSON ({username}.json)
â”‚   â”œâ”€â”€ categories/                 # Zapisane dane kategorii (JSON)
â”‚   â”œâ”€â”€ exports/                    # Eksporty CSV
â”‚   â””â”€â”€ images/                     # Pobrane obrazy
â”‚
â”œâ”€â”€ logs/                           # Logi scrapera
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fixtures/                      # Pliki danych testowych (HTML, JSON, XML)
â”‚   â”‚   â”œâ”€â”€ html/                      # HTML fixtures dla rÃ³Å¼nych typÃ³w stron
â”‚   â”‚   â”‚   â”œâ”€â”€ products/              # Strony produktÃ³w
â”‚   â”‚   â”‚   â”œâ”€â”€ creators/              # Profile twÃ³rcÃ³w
â”‚   â”‚   â”‚   â”œâ”€â”€ categories/            # Strony kategorii
â”‚   â”‚   â”‚   â””â”€â”€ sitemap/               # Pliki sitemap.xml
â”‚   â”‚   â”œâ”€â”€ json/                      # JSON fixtures dla rÃ³Å¼nych scenariuszy
â”‚   â”‚   â”‚   â”œâ”€â”€ products/              # Dane produktÃ³w
â”‚   â”‚   â”‚   â”œâ”€â”€ creators/              # Dane twÃ³rcÃ³w
â”‚   â”‚   â”‚   â””â”€â”€ categories/            # Dane kategorii
â”‚   â”‚   â””â”€â”€ README.md                  # Dokumentacja fixture'Ã³w
â”‚   â”œâ”€â”€ conftest.py                    # GÅ‚Ã³wne fixture'y wspÃ³Å‚dzielone
â”‚   â”œâ”€â”€ test_scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conftest.py                # Fixture'y specyficzne dla scrapers
â”‚   â”‚   â””â”€â”€ test_sitemap_scraper.py
â”‚   â”œâ”€â”€ test_parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conftest.py                # Fixture'y specyficzne dla parsers
â”‚   â”‚   â””â”€â”€ test_product_parser.py
â”‚   â”œâ”€â”€ test_models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conftest.py                # Fixture'y specyficzne dla models
â”‚   â”‚   â””â”€â”€ test_product.py
â”‚   â””â”€â”€ test_utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conftest.py                # Fixture'y specyficzne dla utils
â”‚       â””â”€â”€ test_normalizers.py

**ğŸ“š Dokumentacja testÃ³w:** Zobacz [`TESTING_AND_FIXTURES.md`](./TESTING_AND_FIXTURES.md) dla peÅ‚nej dokumentacji struktury testÃ³w, fixture'Ã³w i best practices.
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py                 # Setup bazy danych (PostgreSQL/MongoDB)
â”‚   â””â”€â”€ export_data.py              # Export danych do CSV
â”‚   # clean_data.py - nie zaimplementowane (opcjonalne)
â”‚
â”‚   # docs/ - folder nie istnieje (dokumentacja w gÅ‚Ã³wnym katalogu)
â”‚
â”œâ”€â”€ .env.example                    # PrzykÅ‚adowe zmienne Å›rodowiskowe
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml                   # Python project config (poetry/pip)
â”œâ”€â”€ requirements.txt                 # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ requirements-dev.txt             # ZaleÅ¼noÅ›ci dev
â”œâ”€â”€ pytest.ini                       # Konfiguracja pytest
â”œâ”€â”€ cursor documentation and rules/  # Dokumentacja techniczna
â”‚   â”œâ”€â”€ REKOMENDACJE_SCRAPERA_FRAMER.md
â”‚   â”œâ”€â”€ PROPOZYCJA_ARCHITEKTURY.md
â”‚   â””â”€â”€ STACK_TECHNICZNY.md
```

## ğŸ”§ Komponenty Systemu

### 1. Scrapers (`src/scrapers/`)

#### `marketplace_scraper.py`
- GÅ‚Ã³wny orchestrator scrapowania
- Koordynuje pracÄ™ innych scraperÃ³w
- ZarzÄ…dza kolejkÄ… URL-i do przetworzenia
- ObsÅ‚uguje resume capability

#### `sitemap_scraper.py`
- Pobiera i parsuje `sitemap.xml` (marketplace lub gÅ‚Ã³wny sitemap)
- WyodrÄ™bnia wszystkie URL-e:
  - **Produkty**: templates, components, vectors, **plugins** â­
  - **Kategorie**: `/marketplace/category/{nazwa}/`
  - **Profile**: `/@{username}/` (wszystko zaczynajÄ…ce siÄ™ od `@`)
  - **Strony pomocowe**: `/help/articles/...marketplace...`
- Filtruje wedÅ‚ug typu produktu
- ObsÅ‚uguje fallback jeÅ›li marketplace sitemap nie dziaÅ‚a

#### `product_scraper.py`
- Scrapuje pojedynczy produkt (template/component/vector/**plugin**)
- ObsÅ‚uguje wszystkie typy produktÃ³w:
  - Templates: `/marketplace/templates/{nazwa}/`
  - Components: `/marketplace/components/{nazwa}/`
  - Vectors: `/marketplace/vectors/{nazwa}/`
  - **Plugins**: `/marketplace/plugins/{nazwa}/` â­
- Pobiera stronÄ™ produktu
- WywoÅ‚uje parsery do ekstrakcji danych

#### `creator_scraper.py`
- Scrapuje profil twÃ³rcy (`/@username/`)
- **UWAGA**: Username moÅ¼e zawieraÄ‡ znaki specjalne (np. `/@-790ivi/`)
- ObsÅ‚uguje wszystkie profile zaczynajÄ…ce siÄ™ od `@`
- Pobiera statystyki twÃ³rcy
- Zbiera informacje o wszystkich produktach twÃ³rcy (templates/components/vectors/plugins)

### 2. Parsers (`src/parsers/`)

#### `product_parser.py`
- Parsuje HTML strony produktu
- ObsÅ‚uguje wszystkie typy produktÃ³w (templates/components/vectors/**plugins**)
- Ekstrahuje: nazwa, cena, opis, funkcje, obrazy, typ produktu, kategorie
- UÅ¼ywa selektorÃ³w CSS z dokumentacji
- Identyfikuje typ produktu z URL lub HTML
- **Components Installs**: WyciÄ…gane z JSON danych Next.js (priorytet) lub z HTML tekstu. MoÅ¼e byÄ‡ niedostÄ™pne dla niektÃ³rych komponentÃ³w.
- Dekodowanie URL-i obrazÃ³w Next.js do oryginalnych URL-i

#### `category_parser.py`
- Parsuje stronÄ™ kategorii
- Ekstrahuje: nazwa kategorii, opis, liczbÄ™ produktÃ³w, typy produktÃ³w, subkategorie
- **find_product_position()**: Znajduje pozycjÄ™ produktu w kategorii (od lewej do prawej, od gÃ³ry do doÅ‚u, 1-indexed). Tylko dla szablonÃ³w.

#### `creator_parser.py`
- Parsuje profil twÃ³rcy
- Ekstrahuje: statystyki, produkty, bio, social media, avatar
- Profil jest zapisywany jako osobny plik JSON: `data/creators/{username}.json`
- **Avatar**: WyciÄ…gany z JSON danych Next.js (priorytet), pomijane placeholdery API
- **Social Media**: WyciÄ…gane z JSON danych Next.js, automatycznie filtrowane linki Framer. ObsÅ‚ugiwane: Twitter/X, LinkedIn, Instagram, GitHub, Dribbble, Behance, YouTube


### 3. Models (`src/models/`)

#### Pydantic Models

**Product Model (Normalizacja):**
- **NormalizedDate:** Format daty z surowym i znormalizowanym formatem
  - `raw`: Format surowy z HTML (np. "5 months ago", "3mo ago")
  - `normalized`: ISO 8601 (np. "2024-10-15T00:00:00Z")
- **NormalizedStatistic:** Format statystyki z surowym i znormalizowanym formatem
  - `raw`: Format surowy z HTML (np. "19.8K Views", "1,200 Vectors")
  - `normalized`: Liczba caÅ‚kowita (np. 19800, 1200)
- **ProductStats:** Statystyki produktu (rÃ³Å¼ne dla rÃ³Å¼nych typÃ³w)
  - `views`, `pages`, `users`, `installs`, `vectors`
  - Wszystkie jako `NormalizedStatistic`
- **ProductMetadata:** Metadane produktu
  - `published_date`, `last_updated` jako `NormalizedDate`
  - `version` (string, dla plugins)
- **Product:** GÅ‚Ã³wny model produktu
  - Typ produktu: `template`, `component`, `vector`, **`plugin`** â­
  - ObsÅ‚uga wszystkich pÃ³l z dokumentacji
  - Wszystkie daty i statystyki w formacie znormalizowanym
  - **category_positions**: Pozycja produktu w kaÅ¼dej kategorii (Dict[str, int]) - tylko dla szablonÃ³w

**Creator Model:**
- **Creator:** Walidacja danych twÃ³rcy
  - Username moÅ¼e zawieraÄ‡ znaki specjalne (np. `/@-790ivi/`)
  - Lista produktÃ³w (templates/components/vectors/plugins)

**Category Model:**
- **Category:** Walidacja danych kategorii
  - Nazwa, URL, opis, lista produktÃ³w

**Automatyczna serializacja:** Wszystkie modele automatycznie serializujÄ… do JSON

**UWAGA:** Recenzje nie sÄ… dostÄ™pne na Framer Marketplace, wiÄ™c nie sÄ… zbierane.

### 4. Storage (`src/storage/`)

#### `file_storage.py`
- Zapis produktÃ³w do JSON (jeden plik per produkt: `products/{type}/{product_id}.json`)
- Zapis kreatorÃ³w do JSON (jeden plik per kreator: `creators/{username}.json`)
- Zapis kategorii do JSON (jeden plik per kategoria: `categories/{slug}.json`)
- Eksport produktÃ³w do CSV (`export_products_to_csv()`)
- Eksport kreatorÃ³w do CSV (`export_creators_to_csv()`)
- **Zawsze nadpisuje pliki** - produkty sÄ… zawsze aktualizowane z najnowszymi danymi (views, ceny, stats)
- Dodaje timestamp `scraped_at` do kaÅ¼dego produktu

### 5. Utils (`src/utils/`)

#### `rate_limiter.py`
- Ograniczenie do 1-2 req/s
- Randomizacja opÃ³ÅºnieÅ„
- Respektowanie robots.txt

#### `retry.py`
- Exponential backoff
- ObsÅ‚uga timeoutÃ³w
- Retry logic dla failed requests

#### `logger.py`
- Strukturalne logowanie (structlog)
- RÃ³Å¼ne poziomy logowania
- Rotacja logÃ³w

#### `normalizers.py` â­
- **Normalizacja dat**:
  - `parse_relative_date()`: Konwertuje "X months ago" â†’ ISO 8601
  - ObsÅ‚uguje formaty: "X months ago", "Xmo ago", "Xw ago", "X days ago"
  - Zwraca: `{"raw": "...", "normalized": "ISO 8601"}`
- **Normalizacja statystyk**:
  - `parse_statistic()`: Konwertuje "19.8K Views" â†’ 19800
  - ObsÅ‚uguje formaty: "X.XK", "XK", "X,XXX", "XXX"
  - Zwraca: `{"raw": "...", "normalized": int}`
- **UÅ¼ycie:** Parser wywoÅ‚uje normalizatory przed zapisem do modelu

#### `checkpoint.py`
- System checkpoint do zapisywania postÄ™pu scrapowania
- Zapisuje przetworzone URL-e i nieudane URL-e
- **Automatyczny retry failed URLs** - na koÅ„cu scrapowania ponawia prÃ³bÄ™ dla nieudanych URL-i
- Zapis do `data/checkpoint.json`
- **Uwaga**: Checkpoint sÅ‚uÅ¼y gÅ‚Ã³wnie do Å›ledzenia bÅ‚Ä™dÃ³w i retry, nie do pomijania juÅ¼ przetworzonych (produkty sÄ… zawsze aktualizowane)

#### `metrics.py`
- Tracking metryk scrapowania
- Åšledzi: liczbÄ™ produktÃ³w, kreatorÃ³w, kategorii, czas wykonania, success rate
- Logowanie podsumowania po zakoÅ„czeniu scrapowania

## ğŸ”„ Flow Scrapowania

```
1. START
   â”‚
   â”œâ”€â–¶ main.py (entry point)
   â”‚
2. INITIALIZATION
   â”œâ”€â–¶ Wczytaj konfiguracjÄ™ (.env)
   â”œâ”€â–¶ SprawdÅº robots.txt
   â”œâ”€â–¶ Inicjalizuj logger
   â””â”€â–¶ Przygotuj sesjÄ™ HTTP
   â”‚
3. GET PRODUCT LIST
   â”œâ”€â–¶ sitemap_scraper.py â†’ pobierz sitemap.xml
   â”‚   â”œâ”€â–¶ SprÃ³buj: `/marketplace/sitemap.xml`
   â”‚   â””â”€â–¶ Fallback: `/sitemap.xml` (jeÅ›li marketplace nie dziaÅ‚a)
   â”œâ”€â–¶ WyodrÄ™bnij wszystkie URL-e:
   â”‚   â”œâ”€â–¶ Produkty:
   â”‚   â”‚   â”œâ”€â–¶ Templates: `/marketplace/templates/{nazwa}/`
   â”‚   â”‚   â”œâ”€â–¶ Components: `/marketplace/components/{nazwa}/`
   â”‚   â”‚   â”œâ”€â–¶ Vectors: `/marketplace/vectors/{nazwa}/`
   â”‚   â”‚   â””â”€â–¶ Plugins: `/marketplace/plugins/{nazwa}/` â­
   â”‚   â”œâ”€â–¶ Kategorie: `/marketplace/category/{nazwa}/`
   â”‚   â”œâ”€â–¶ Profile: `/@{username}/` (wszystko z `@`)
   â”‚   â””â”€â–¶ Strony pomocowe: `/help/articles/...marketplace...`
   â””â”€â–¶ Filtruj wedÅ‚ug typu (templates/components/vectors/plugins)
   â”‚
4. SCRAPE PRODUCTS
   â”œâ”€â–¶ Dla kaÅ¼dego produktu (rÃ³wnolegle z limitem):
   â”‚   â”œâ”€â–¶ product_scraper.py â†’ pobierz stronÄ™ produktu
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje: templates/components/vectors/**plugins** â­
   â”‚   â”œâ”€â–¶ product_parser.py â†’ ekstrahuj dane
   â”‚   â”‚   â””â”€â–¶ Identyfikuj typ produktu (template/component/vector/plugin)
   â”‚   â”œâ”€â–¶ creator_scraper.py â†’ pobierz profil twÃ³rcy (`/@username/`)
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje username z znakami specjalnymi
   â”‚   â”œâ”€â–¶ creator_parser.py â†’ ekstrahuj dane twÃ³rcy
   â”‚   â”œâ”€â–¶ save_creator_json() â†’ zapisz profil twÃ³rcy jako osobny plik (data/creators/{username}.json)
   â”‚   â”œâ”€â–¶ (Tylko dla szablonÃ³w) Dla kaÅ¼dej kategorii produktu:
   â”‚   â”‚   â”œâ”€â–¶ category_scraper.py â†’ pobierz stronÄ™ kategorii
   â”‚   â”‚   â”œâ”€â–¶ category_parser.find_product_position() â†’ znajdÅº pozycjÄ™ produktu
   â”‚   â”‚   â””â”€â–¶ Zapisz pozycjÄ™ w product.category_positions[category]
   â”‚   â”œâ”€â–¶ Walidacja danych (Pydantic)
   â”‚   â”œâ”€â–¶ Zapis danych (file_storage.py)
   â”‚   â”‚   â”œâ”€â–¶ Zapis produktu: products/{type}/{product_id}.json (zawsze nadpisuje - aktualizuje views, ceny, stats)
   â”‚   â”‚   â””â”€â–¶ Zapis kreatora: creators/{username}.json (osobny plik)
   â”‚   â”œâ”€â–¶ Aktualizacja checkpoint (checkpoint.py)
   â”‚   â””â”€â–¶ Na koÅ„cu scrapowania: Retry failed URLs (ponowna prÃ³ba dla nieudanych URL-i)
   â”‚
4b. SCRAPE CATEGORIES
   â”œâ”€â–¶ Dla kaÅ¼dej kategorii z sitemap:
   â”‚   â”œâ”€â–¶ category_scraper.py â†’ pobierz `/marketplace/category/{nazwa}/`
   â”‚   â”œâ”€â–¶ category_parser.py â†’ ekstrahuj:
   â”‚   â”‚   â”œâ”€â–¶ Nazwa kategorii
   â”‚   â”‚   â”œâ”€â–¶ Opis kategorii
   â”‚   â”‚   â”œâ”€â–¶ Lista produktÃ³w w kategorii
   â”‚   â”‚   â””â”€â–¶ Liczba produktÃ³w
   â”‚   â””â”€â–¶ Zapis danych kategorii
   â”‚
4c. SCRAPE PROFILES
   â”œâ”€â–¶ Dla kaÅ¼dego profilu z sitemap (`/@username/`):
   â”‚   â”œâ”€â–¶ creator_scraper.py â†’ pobierz profil
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje username z znakami specjalnymi (np. `/@-790ivi/`)
   â”‚   â”œâ”€â–¶ creator_parser.py â†’ ekstrahuj:
   â”‚   â”‚   â”œâ”€â–¶ Username (z URL)
   â”‚   â”‚   â”œâ”€â–¶ Nazwa wyÅ›wietlana
   â”‚   â”‚   â”œâ”€â–¶ Bio/opis
   â”‚   â”‚   â”œâ”€â–¶ Avatar
   â”‚   â”‚   â”œâ”€â–¶ Lista produktÃ³w (templates/components/vectors/plugins)
   â”‚   â”‚   â”œâ”€â–¶ Statystyki
   â”‚   â”‚   â””â”€â–¶ Linki do social media
   â”‚   â””â”€â–¶ Zapis danych profilu
   â”‚
5. POST-PROCESSING
   â”œâ”€â–¶ Czyszczenie danych
   â”œâ”€â–¶ Normalizacja danych (Opcja B) â­
   â”‚   â”œâ”€â–¶ normalizers.py â†’ parse_relative_date() dla dat
   â”‚   â””â”€â–¶ normalizers.py â†’ parse_statistic() dla statystyk
   â”œâ”€â–¶ Weryfikacja kompletnoÅ›ci
   â”œâ”€â–¶ Dekodowanie URL-i obrazÃ³w (Next.js Image â†’ oryginalny URL)
   â””â”€â–¶ Generowanie raportÃ³w
   â”‚
6. SAVE & BACKUP
   â”œâ”€â–¶ Zapis do JSON/CSV (file_storage.py)
   â”‚   â””â”€â–¶ Zawsze nadpisuje pliki - aktualizuje views, ceny, stats
   â”œâ”€â–¶ Zapis checkpoint (checkpoint.py)
   â”œâ”€â–¶ Retry failed URLs (ponowna prÃ³ba dla nieudanych URL-i)
   â”‚   â””â”€â–¶ Z niÅ¼szÄ… wspÃ³Å‚bieÅ¼noÅ›ciÄ… (max 3 concurrent) aby nie przeciÄ…Å¼aÄ‡ serwera
   â”œâ”€â–¶ Logowanie metryk (metrics.py)
   â””â”€â–¶ Backup (GitHub Actions artifacts)
   â”‚
7. END
```

## ğŸš€ Deployment Strategy

### GitHub Actions Workflows

#### 1. `.github/workflows/scrape.yml`
```yaml
name: Daily Scrape
on:
  schedule:
    - cron: '0 2 * * *'  # Codziennie o 2:00 UTC
  workflow_dispatch:     # RÄ™czne uruchomienie

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python src/main.py
      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: data/
```

#### 2. `.github/workflows/ci.yml`
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - name: Install dependencies
        run: pip install -r requirements-dev.txt
      - name: Run tests
        run: pytest
      - name: Lint
        run: ruff check .
      - name: Type check
        run: mypy src/
```


## ğŸ“Š Monitoring & Logging

### Logging Structure
```python
# PrzykÅ‚ad uÅ¼ycia structlog
logger.info("scraping_started", product_count=1000)
logger.info("product_scraped", product_id="123", status="success")
logger.error("scraping_failed", error="timeout", retry_count=2)
```

### Metrics to Track
- Liczba przetworzonych produktÃ³w
- Liczba bÅ‚Ä™dÃ³w
- Czas scrapowania
- Success rate
- Rate limit violations

### Notifications
- GitHub Actions: email/Slack o statusie

## ğŸ” Security & Best Practices

### Environment Variables
```bash
# .env.example
DATABASE_URL=postgresql://...
FRAMER_BASE_URL=https://www.framer.com
MARKETPLACE_SITEMAP_URL=https://www.framer.com/marketplace/sitemap.xml
MAIN_SITEMAP_URL=https://www.framer.com/sitemap.xml  # Fallback
RATE_LIMIT=1.0
MAX_RETRIES=3
LOG_LEVEL=INFO
```

### Rate Limiting
- 1-2 requestÃ³w na sekundÄ™
- Randomizacja opÃ³ÅºnieÅ„ (0.5-2s)
- Respektowanie robots.txt

### Error Handling
- Retry z exponential backoff
- Timeout handling
- Graceful degradation
- Checkpoint system (resume capability)

### Data Validation
- Pydantic models dla wszystkich danych
- Walidacja przed zapisem
- Sprawdzanie wymaganych pÃ³l

### Data Normalization â­
- **Normalizacja dat**: Relatywne daty ("X months ago") â†’ ISO 8601
  - Format: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
  - Funkcja: `utils/normalizers.py::parse_relative_date()`
- **Normalizacja statystyk**: SkrÃ³cone formaty ("19.8K") â†’ liczby caÅ‚kowite
  - Format: `{"raw": "19.8K Views", "normalized": 19800}`
  - Funkcja: `utils/normalizers.py::parse_statistic()`
- **Zapis obu formatÃ³w**: Zapewnia weryfikacjÄ™ i elastycznoÅ›Ä‡ analizy
- **Modele Pydantic**: `NormalizedDate`, `NormalizedStatistic` w modelach produktu

## ğŸ¯ Next Steps

### Faza 1: Setup (MVP)
1. âœ… StwÃ³rz strukturÄ™ projektu
2. âœ… Setup Python environment (poetry/pip)
3. âœ… Implementuj podstawowy scraper (sitemap â†’ products)
4. âœ… Implementuj rate limiting i error handling
5. âœ… Implementuj normalizacjÄ™ danych â­
   - `utils/normalizers.py` z funkcjami parse_relative_date() i parse_statistic()
   - Modele Pydantic z NormalizedDate i NormalizedStatistic
6. âœ… Test na maÅ‚ej prÃ³bce (10-20 produktÃ³w)

### Faza 2: Rozszerzenie
1. âœ… Dodaj scraping wszystkich typÃ³w produktÃ³w (templates/components/vectors/**plugins**)
2. âœ… Dodaj scraping twÃ³rcÃ³w
3. âœ… Dodaj scraping kategorii
4. âœ… Setup GitHub Actions
5. âœ… Monitoring i metryki

---

*Dokument wygenerowany na podstawie REKOMENDACJE_SCRAPERA_FRAMER.md*

