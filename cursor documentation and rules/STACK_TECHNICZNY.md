# Stack Techniczny - Scraper Framer Marketplace

## üìã PrzeglƒÖd Projektu

Projekt to zaawansowany scraper do zbierania danych z Framer Marketplace, kt√≥ry umo≈ºliwia automatyzacjƒô pobierania informacji o:

- **Produktach**: Szablony (templates), Komponenty (components), Wektory (vectors), **Wtyczki (plugins)** ‚≠ê
- **Tw√≥rcach/U≈ºytkownikach**: Profile z username (mo≈ºe zawieraƒá znaki specjalne)
- **Kategoriach**: Kategorie produkt√≥w w marketplace

**Kluczowa funkcjonalno≈õƒá:** Normalizacja danych - zapis zar√≥wno format√≥w surowych z HTML jak i znormalizowanych (ISO 8601 dla dat, liczby ca≈Çkowite dla statystyk) dla elastyczno≈õci analizy i weryfikacji danych.

## üõ†Ô∏è Stack Techniczny

### Backend / Scraper Core

#### Jƒôzyk Programowania
- **Python 3.11+** 
  - Nowoczesne funkcje (pattern matching, type hints)
  - Doskona≈Ça obs≈Çuga asynchroniczno≈õci
  - Bogaty ekosystem bibliotek

#### Biblioteki Core

**HTTP Client & Scraping:**
- **httpx** (async) lub **requests** (sync)
  - Asynchroniczne requesty dla lepszej wydajno≈õci
  - Obs≈Çuga sesji, cookies, headers
  - Wsparcie dla retry logic
  
- **BeautifulSoup4** (bs4)
  - Parsowanie HTML
  - ≈Åatwe wyszukiwanie selektor√≥w CSS
  
- **lxml**
  - Szybsze parsowanie XML/HTML dla sitemap

**Dane & Modele:**
- **pydantic v2**
  - Walidacja danych z modelem
  - Type safety
  - Automatyczna serializacja do JSON
  - Modele z normalizacjƒÖ:
    - `NormalizedDate` - daty z formatem surowym i znormalizowanym
    - `NormalizedStatistic` - statystyki z formatem surowym i znormalizowanym
  
- **pandas**
  - Manipulacja i analiza danych
  - Eksport do CSV/Excel
  
- **SQLAlchemy 2.0** (opcjonalnie)
  - ORM dla bazy danych relacyjnej
  - Wsparcie dla PostgreSQL/MySQL

**Narzƒôdzia Pomocnicze:**
- **python-dotenv**
  - ZarzƒÖdzanie zmiennymi ≈õrodowiskowymi
  
- **tqdm**
  - Pasek postƒôpu dla d≈Çugich operacji
  
- **tenacity**
  - Retry logic z exponential backoff
  
- **fake-useragent**
  - Rotacja User-Agent headers

- **aiofiles** (je≈õli async)
  - Asynchroniczne operacje na plikach

**Logging & Monitoring:**
- **structlog**
  - Strukturalne logowanie
  - ≈Åatwa integracja z systemami monitoringu

### Baza Danych

#### PostgreSQL (Rekomendowana)
- **Dlaczego:** Najlepsze dla relacyjnych danych (produkty ‚Üî tw√≥rcy ‚Üî recenzje)
- **ORM:** SQLAlchemy
- **Hosting:** 
  - Supabase (darmowy tier)
  - Railway
  - Neon (serverless PostgreSQL)
  - Vercel Postgres (integracja z Vercel)

#### Opcja 2: SQLite (Dla ma≈Çych projekt√≥w)
- **Dlaczego:** Zero konfiguracji, plik lokalny
- **ORM:** SQLAlchemy
- **Limitations:** Nie nadaje siƒô dla du≈ºych danych

#### MongoDB (Dla dokument√≥w)
- **Dlaczego:** Elastyczny schemat, ≈Çatwe przechowywanie JSON
- **Driver:** pymongo lub motor (async)
- **Hosting:** MongoDB Atlas (darmowy tier)

#### Tylko Pliki (JSON/CSV)
- **Dlaczego:** Najprostsze, brak infrastruktury
- **Formaty:** JSON, CSV, Parquet (dla analiz)

### Storage & Hosting

#### GitHub Actions (CI/CD & Automatyzacja) ‚úÖ

**Zastosowanie:**
- **Scheduled Scraping:** Automatyczne uruchamianie scrapowania (np. codziennie o 2:00)
- **CI/CD:** Testy automatyczne przed merge
- **Data Backup:** Automatyczny backup danych do GitHub Releases/Artifacts
- **Monitoring:** Wysy≈Çanie notyfikacji o statusie scrapowania

**Przyk≈Çadowe workflow:**
```yaml
# .github/workflows/scrape.yml
name: Daily Scrape
on:
  schedule:
    - cron: '0 2 * * *'  # Codziennie o 2:00 UTC
  workflow_dispatch:  # Rƒôczne uruchomienie
```

**Zalety:**
- ‚úÖ Darmowe dla publicznych repozytori√≥w
- ‚úÖ 2000 minut/miesiƒÖc dla private repos
- ‚úÖ Integracja z GitHub
- ‚úÖ Automatyczne uruchamianie
- ‚úÖ Mo≈ºliwo≈õƒá zapisywania artifacts (dane)

**Ograniczenia:**
- ‚ö†Ô∏è Czas wykonania: max 6 godzin
- ‚ö†Ô∏è Limit czasu dla scheduled workflows

#### Vercel (Hosting & API) ‚ö†Ô∏è

**Zastosowanie:**
- **API Endpoints:** REST API do dostƒôpu do danych (je≈õli potrzebny)
- **Dashboard/Frontend:** Webowy interfejs do przeglƒÖdania danych
- **Serverless Functions:** Lekkie endpointy do zapyta≈Ñ

**Zalety:**
- ‚úÖ Darmowe dla hobby projects
- ‚úÖ Serverless (automatyczne skalowanie)
- ‚úÖ Integracja z GitHub (auto-deploy)
- ‚úÖ Edge Functions (szybkie API)
- ‚úÖ Vercel Postgres (je≈õli potrzebna baza)

**Ograniczenia:**
- ‚ö†Ô∏è Vercel jest g≈Ç√≥wnie dla Node.js/Python (ograniczenia dla Python)
- ‚ö†Ô∏è Funkcje serverless majƒÖ limit czasu (10s hobby, 60s pro)
- ‚ö†Ô∏è Nie idealne do d≈Çugotrwa≈Çych scrapowania

**Rekomendacja:**
- ‚úÖ **U≈ºyj Vercel** je≈õli potrzebujesz:
  - API do dostƒôpu do danych
  - Dashboard/frontend
  - Integracjƒô z Vercel Postgres
- ‚ùå **NIE u≈ºywaj Vercel** do:
  - G≈Ç√≥wnego procesu scrapowania (u≈ºyj GitHub Actions lub dedykowany serwer)
  - D≈Çugotrwa≈Çych operacji (>10s)

**Alternatywy dla Vercel (je≈õli potrzebny API):**
- **FastAPI + Railway/Render** (Python-native)
- **Flask + Heroku** (prostsze, ale dro≈ºsze)
- **Next.js API Routes** (je≈õli frontend w Next.js)

### Struktura Deployment

#### Opcja 1: GitHub Actions (Rekomendowana dla startu)
```
GitHub Actions (scheduled)
  ‚Üì
  Scraper Python
  ‚Üì
  Zapis do JSON/CSV
  ‚Üì
  GitHub Artifacts / Releases
  ‚Üì
  (Opcjonalnie) Push do bazy danych
```

**Zalety:**
- ‚úÖ Zero koszt√≥w (dla public repos)
- ‚úÖ Automatyzacja out-of-the-box
- ‚úÖ Integracja z GitHub

```
GitHub Actions (scraping)
  ‚Üì
  Zapis do bazy (Vercel Postgres / Supabase)
  ‚Üì
  Vercel API (dostƒôp do danych)
  ‚Üì
  Frontend (Vercel) - dashboard
```

**Zalety:**
- ‚úÖ Automatyzacja scrapowania
- ‚úÖ API do danych
- ‚úÖ Dashboard dla u≈ºytkownik√≥w

#### Opcja 3: Dedicated Server (VPS/Cloud)
- **Railway** - ≈Çatwa konfiguracja, darmowy tier
- **Render** - darmowy tier z limitami
- **DigitalOcean App Platform** - p≈Çatne, ale elastyczne
- **AWS EC2 / Lambda** - zaawansowane, ale bardziej skomplikowane

### Narzƒôdzia Deweloperskie

#### Development
- **poetry** lub **pip-tools** - zarzƒÖdzanie zale≈ºno≈õciami
- **pre-commit** - hooks przed commit (formatting, linting)
- **black** - formatowanie kodu
- **ruff** lub **pylint** - linting
- **mypy** - type checking

#### Testing
- **pytest** - framework testowy
- **pytest-asyncio** - testy async
- **pytest-cov** - coverage
- **httpx mock** - mockowanie request√≥w HTTP

#### Monitoring & Alerting
- **Sentry** (opcjonalnie) - error tracking
- **GitHub Actions notifications** - email/Slack o statusie
- **Health checks** - monitoring scrapowania

### Rekomendowana Architektura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GitHub Repository                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  GitHub Actions  ‚îÇ         ‚îÇ   Vercel (API)   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  (Scheduled)     ‚îÇ         ‚îÇ   (Opcjonalnie) ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ         ‚îÇ                  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Scraping      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  - REST API      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Data Storage  ‚îÇ         ‚îÇ  - Dashboard     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Backup        ‚îÇ         ‚îÇ  - Frontend      ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ           ‚îÇ                            ‚îÇ                 ‚îÇ
‚îÇ           ‚ñº                            ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Data Storage    ‚îÇ         ‚îÇ  Database        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ         ‚îÇ  (Opcjonalnie)   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - JSON/CSV      ‚îÇ         ‚îÇ  - PostgreSQL    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - GitHub        ‚îÇ         ‚îÇ  - Supabase      ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ    Artifacts     ‚îÇ         ‚îÇ  - Vercel Postgres‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Rekomendacja Startowa

### Faza 1: MVP (Minimum Viable Product)
1. **Backend:** Python scraper z httpx + BeautifulSoup
2. **Storage:** JSON/CSV files
3. **Automation:** GitHub Actions (scheduled)
4. **Deployment:** Zero (lokalne uruchomienie lub GitHub Actions)

### Faza 2: Rozszerzenie
1. **Database:** PostgreSQL (Supabase/Railway)
2. **API:** FastAPI + Railway (lub Vercel je≈õli Next.js)
3. **Monitoring:** GitHub Actions notifications

### Faza 3: Production
1. **Frontend:** Next.js dashboard (Vercel)
2. **API:** Vercel API Routes lub FastAPI
3. **Database:** Vercel Postgres lub Supabase
4. **Monitoring:** Sentry + Health checks

## üì¶ Zalecane Zale≈ºno≈õci (requirements.txt)

```txt
# HTTP & Scraping
httpx>=0.25.0
beautifulsoup4>=4.12.0
lxml>=5.0.0

# Data & Validation
pydantic>=2.5.0
pandas>=2.1.0
python-dotenv>=1.0.0

# Utilities
tqdm>=4.66.0
tenacity>=8.2.0
fake-useragent>=1.4.0
structlog>=23.2.0

# Database (opcjonalnie)
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.9  # PostgreSQL
# lub
pymongo>=4.6.0  # MongoDB

# Development
pytest>=7.4.0
pytest-asyncio>=0.21.0
black>=23.11.0
ruff>=0.1.0
mypy>=1.7.0
```

## üéØ Podsumowanie

### GitHub Actions ‚úÖ
**U≈ºyj do:**
- Automatycznego scrapowania (scheduled)
- CI/CD
- Backup danych
- Notyfikacji

### Vercel ‚ö†Ô∏è
**U≈ºyj do:**
- API endpoints (je≈õli potrzebny dostƒôp do danych)
- Frontend/dashboard (je≈õli potrzebny)
- Integracji z Vercel Postgres

**Nie u≈ºywaj do:**
- G≈Ç√≥wnego procesu scrapowania (za d≈Çugie operacje)

### Rekomendacja
**Start:** GitHub Actions + JSON/CSV storage  
**Rozw√≥j:** + Database (Supabase/Railway) + API (FastAPI)  
**Production:** + Vercel (dashboard) + Monitoring

---

*Ostatnia aktualizacja: 2024-12-19*

