name: Sync JSON to Database

on:
  workflow_dispatch:  # Rƒôczne uruchomienie
  schedule:
    - cron: '0 3 * * *'  # Codziennie o 3:00 UTC (po scrapowaniu o 2:00)

jobs:
  sync:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Find and download latest scrape artifacts
        id: download-artifacts
        continue-on-error: true
        run: |
          echo "üîç Looking for latest successful 'Daily Scrape' workflow run..."
          
          # Get latest successful run of scrape workflow
          RUN_ID=$(gh run list --workflow=scrape.yml --status=success --limit=1 --json databaseId --jq '.[0].databaseId' 2>/dev/null || echo "")
          
          if [ -z "$RUN_ID" ] || [ "$RUN_ID" == "null" ]; then
            echo "‚ö†Ô∏è No successful scrape workflow run found"
            echo "downloaded=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "‚úÖ Found scrape workflow run: $RUN_ID"
          
          # List artifacts from this run
          ARTIFACT_ID=$(gh api repos/${{ github.repository }}/actions/runs/$RUN_ID/artifacts --jq '.artifacts[] | select(.name=="scraped-data-latest") | .id' 2>/dev/null || echo "")
          
          if [ -z "$ARTIFACT_ID" ] || [ "$ARTIFACT_ID" == "null" ]; then
            echo "‚ö†Ô∏è Artifact 'scraped-data-latest' not found in run $RUN_ID"
            echo "downloaded=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "‚úÖ Found artifact ID: $ARTIFACT_ID"
          
          # Create data directory
          mkdir -p data
          
          # Download artifact using GitHub API
          echo "üì• Downloading artifact..."
          gh api repos/${{ github.repository }}/actions/artifacts/$ARTIFACT_ID/zip -o /tmp/artifact.zip 2>/dev/null
          
          if [ $? -eq 0 ] && [ -f /tmp/artifact.zip ]; then
            echo "üì¶ Extracting artifact..."
            # Install unzip if not available (should be available by default)
            which unzip || (apt-get update && apt-get install -y unzip)
            unzip -q -o /tmp/artifact.zip -d data/ 2>/dev/null
            rm -f /tmp/artifact.zip
            
            if [ -d "data/products" ] && [ -n "$(ls -A data/products/*/*.json 2>/dev/null)" ]; then
              echo "‚úÖ Artifact downloaded and extracted successfully"
              echo "downloaded=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è Artifact extracted but no product files found"
              echo "downloaded=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå Failed to download artifact"
            echo "downloaded=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}
      
      - name: Check if artifact was downloaded
        run: |
          if [ "${{ steps.download-artifacts.outputs.downloaded }}" != "true" ]; then
            echo "‚ö†Ô∏è Artifact 'scraped-data-latest' not found or download failed"
            echo "This may happen if:"
            echo "  - Scrape workflow hasn't run yet"
            echo "  - Scrape workflow failed"
            echo "  - Artifact expired (retention: 7 days)"
            echo ""
            echo "Checking if data/ directory exists locally..."
            if [ -d "data/products" ] && [ -n "$(ls -A data/products/*/*.json 2>/dev/null)" ]; then
              echo "‚úÖ Found local data files - will use them instead"
            else
              echo "‚ùå No data found. Cannot proceed with sync."
              exit 1
            fi
          else
            echo "‚úÖ Artifact downloaded successfully"
          fi
      
      - name: Check if data exists
        run: |
          if [ ! -d "data/products" ]; then
            echo "‚ùå data/products directory not found"
            echo "Available directories:"
            ls -la data/ 2>/dev/null || echo "data/ directory not found"
            exit 1
          fi
          
          json_count=$(find data/products -name "*.json" -type f 2>/dev/null | wc -l)
          if [ "$json_count" -eq 0 ]; then
            echo "‚ùå No JSON data files found in data/products/"
            echo "Checking for product files:"
            find data/ -name "*.json" -type f 2>/dev/null | head -10 || echo "No JSON files found"
            exit 1
          else
            echo "‚úÖ Found $json_count JSON data files"
            echo "Product types:"
            for type in templates components vectors plugins; do
              count=$(find data/products/$type -name "*.json" -type f 2>/dev/null | wc -l)
              if [ "$count" -gt 0 ]; then
                echo "  - $type: $count files"
              fi
            done
          fi
      
      - name: Sync JSON to Database
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          if [ -z "$DATABASE_URL" ]; then
            echo "‚ö†Ô∏è DATABASE_URL not set in secrets. Skipping sync."
            exit 0
          fi
          python scripts/sync_json_to_db.py
      
      - name: Upload sync logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sync-logs
          path: logs/
          retention-days: 7

