name: Daily Scrape

on:
  schedule:
    - cron: '0 2 * * *'  # Codziennie o 2:00 UTC
  workflow_dispatch:  # Rƒôczne uruchomienie

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # Maximum 6 hours per job (scraper needs ~4h for full scrape)
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download previous checkpoint (if available, for resume)
        id: download-checkpoint
        continue-on-error: true
        run: |
          echo "üîç Checking for previous checkpoint from last run..."
          
          # Try to find last run (successful or not) to resume from
          LAST_RUN_ID=$(gh run list --workflow=scrape.yml --limit=1 --json databaseId,conclusion --jq '.[0] | select(.conclusion == "failure" or .conclusion == "cancelled") | .databaseId' 2>/dev/null || echo "")
          
          if [ -n "$LAST_RUN_ID" ] && [ "$LAST_RUN_ID" != "null" ]; then
            echo "üì• Found previous failed/cancelled run: $LAST_RUN_ID"
            echo "Attempting to download checkpoint..."
            
            # Try to download checkpoint artifact (if it exists)
            gh run download $LAST_RUN_ID --name scraped-data-latest --dir data/ 2>/dev/null || \
            gh run download $LAST_RUN_ID --name scraped-data-${{ github.run_number }} --dir data/ 2>/dev/null || \
            echo "‚ö†Ô∏è Could not download checkpoint from previous run"
            
            # Check if checkpoint file exists
            if [ -f "data/checkpoint.json" ]; then
              echo "‚úÖ Checkpoint found! Will resume from previous run."
              echo "checkpoint_found=true" >> $GITHUB_OUTPUT
              echo "Processed URLs count: $(jq '.processed_urls | length' data/checkpoint.json 2>/dev/null || echo 'unknown')"
            else
              echo "‚ÑπÔ∏è No checkpoint file found - will start fresh scrape"
              echo "checkpoint_found=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ÑπÔ∏è No previous failed run found - starting fresh scrape"
            echo "checkpoint_found=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}
      
      - name: Add random jitter to avoid hitting same CloudFront edge
        run: |
          # Add random delay 0-60 seconds to avoid hitting same CloudFront edge
          JITTER=$((RANDOM % 61))
          echo "‚è±Ô∏è Adding ${JITTER}s jitter before scraping..."
          sleep ${JITTER}
      
      - name: Run scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          FRAMER_BASE_URL: ${{ secrets.FRAMER_BASE_URL || 'https://www.framer.com' }}
          RATE_LIMIT: ${{ secrets.RATE_LIMIT || '1.0' }}
          MAX_RETRIES: ${{ secrets.MAX_RETRIES || '3' }}
          LOG_LEVEL: ${{ secrets.LOG_LEVEL || 'INFO' }}
          CHECKPOINT_ENABLED: ${{ secrets.CHECKPOINT_ENABLED || 'true' }}
        run: |
          python -m src.main
      
      - name: Archive scraped data with date
        run: |
          # Create dated folder for this scrape
          SCRAPE_DATE=$(date +%Y-%m-%d)
          SCRAPE_FOLDER="scraped-data-${SCRAPE_DATE}"
          
          # Copy data to dated folder
          if [ -d "data" ]; then
            mkdir -p "$SCRAPE_FOLDER"
            cp -r data/* "$SCRAPE_FOLDER/" 2>/dev/null || true
            echo "‚úÖ Data archived to $SCRAPE_FOLDER"
          fi
      
      - name: Upload artifacts (dated)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data-${{ github.run_number }}
          path: |
            data/
            scraped-data-*/
            logs/
          retention-days: 90
          if-no-files-found: warn
      
      - name: Upload latest data (for API access)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data-latest
          path: |
            data/
            logs/
          retention-days: 7
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: logs/
          retention-days: 7
      
      - name: Check for upstream failures and send alert
        if: failure() && github.event_name == 'schedule'
        run: |
          # Check if scraper failed with exit code 2 (upstream error)
          if [ "${{ job.status }}" == "failure" ]; then
            echo "‚ö†Ô∏è Scraper failed - checking for upstream errors..."
            
            # Check logs for upstream_unavailable error
            if grep -q "upstream_unavailable" logs/*.log 2>/dev/null || grep -q "sitemap_fetch_failed_5xx" logs/*.log 2>/dev/null; then
              echo "üî¥ Upstream error detected - this may trigger alert if it's the second consecutive failure"
              
              # Check previous run status (this would need GitHub API or artifact check)
              # For now, just log the error - full implementation would require:
              # 1. Storing last run status in artifact
              # 2. Checking previous run on failure
              # 3. Sending webhook if two consecutive failures
              
              # Webhook/Slack notification would go here
              # Example: curl -X POST ${{ secrets.WEBHOOK_URL }} -d '{"text":"Upstream error detected"}'
              
              echo "üì¢ Alert: Upstream service unavailable detected"
            fi
          fi
      
      - name: Send webhook notification on failure
        if: failure() && github.event_name == 'schedule'
        env:
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
        run: |
          # Send webhook notification about failure (only if WEBHOOK_URL is set)
          if [ -n "$WEBHOOK_URL" ]; then
            curl -X POST "$WEBHOOK_URL" \
              -H "Content-Type: application/json" \
              -d "{
                \"text\": \"üî¥ Scraper failed\",
                \"attachments\": [{
                  \"color\": \"danger\",
                  \"fields\": [{
                    \"title\": \"Workflow\",
                    \"value\": \"${{ github.workflow }}\",
                    \"short\": true
                  }, {
                    \"title\": \"Run Number\",
                    \"value\": \"${{ github.run_number }}\",
                    \"short\": true
                  }, {
                    \"title\": \"Commit\",
                    \"value\": \"${{ github.sha }}\",
                    \"short\": true
                  }, {
                    \"title\": \"View Run\",
                    \"value\": \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\",
                    \"short\": false
                  }]
                }]
              }" || echo "Webhook notification failed"
          else
            echo "‚ÑπÔ∏è WEBHOOK_URL not set, skipping notification"
          fi

