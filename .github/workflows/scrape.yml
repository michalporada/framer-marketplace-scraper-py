name: Daily Scrape

on:
  schedule:
    - cron: '0 2 * * *'  # Codziennie o 2:00 UTC
  workflow_dispatch:  # RÄ™czne uruchomienie

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Add random jitter to avoid hitting same CloudFront edge
        run: |
          # Add random delay 0-60 seconds to avoid hitting same CloudFront edge
          JITTER=$((RANDOM % 61))
          echo "â±ï¸ Adding ${JITTER}s jitter before scraping..."
          sleep ${JITTER}
      
      - name: Run scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          FRAMER_BASE_URL: ${{ secrets.FRAMER_BASE_URL || 'https://www.framer.com' }}
          RATE_LIMIT: ${{ secrets.RATE_LIMIT || '1.0' }}
          MAX_RETRIES: ${{ secrets.MAX_RETRIES || '3' }}
          LOG_LEVEL: ${{ secrets.LOG_LEVEL || 'INFO' }}
          CHECKPOINT_ENABLED: ${{ secrets.CHECKPOINT_ENABLED || 'true' }}
        run: |
          python -m src.main
      
      - name: Archive scraped data with date
        run: |
          # Create dated folder for this scrape
          SCRAPE_DATE=$(date +%Y-%m-%d)
          SCRAPE_FOLDER="scraped-data-${SCRAPE_DATE}"
          
          # Copy data to dated folder
          if [ -d "data" ]; then
            mkdir -p "$SCRAPE_FOLDER"
            cp -r data/* "$SCRAPE_FOLDER/" 2>/dev/null || true
            echo "âœ… Data archived to $SCRAPE_FOLDER"
          fi
      
      - name: Upload artifacts (dated)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data-${{ github.run_number }}
          path: |
            data/
            scraped-data-*/
            logs/
          retention-days: 90
          if-no-files-found: warn
      
      - name: Upload latest data (for API access)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data-latest
          path: |
            data/
            logs/
          retention-days: 7
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: logs/
          retention-days: 7
      
      - name: Check for upstream failures and send alert
        if: failure() && github.event_name == 'schedule'
        run: |
          # Check if scraper failed with exit code 2 (upstream error)
          if [ "${{ job.status }}" == "failure" ]; then
            echo "âš ï¸ Scraper failed - checking for upstream errors..."
            
            # Check logs for upstream_unavailable error
            if grep -q "upstream_unavailable" logs/*.log 2>/dev/null || grep -q "sitemap_fetch_failed_5xx" logs/*.log 2>/dev/null; then
              echo "ðŸ”´ Upstream error detected - this may trigger alert if it's the second consecutive failure"
              
              # Check previous run status (this would need GitHub API or artifact check)
              # For now, just log the error - full implementation would require:
              # 1. Storing last run status in artifact
              # 2. Checking previous run on failure
              # 3. Sending webhook if two consecutive failures
              
              # Webhook/Slack notification would go here
              # Example: curl -X POST ${{ secrets.WEBHOOK_URL }} -d '{"text":"Upstream error detected"}'
              
              echo "ðŸ“¢ Alert: Upstream service unavailable detected"
            fi
          fi
      
      - name: Send webhook notification on failure
        if: failure() && github.event_name == 'schedule' && secrets.WEBHOOK_URL != ''
        run: |
          # Send webhook notification about failure
          curl -X POST "${{ secrets.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d "{
              \"text\": \"ðŸ”´ Scraper failed\",
              \"attachments\": [{
                \"color\": \"danger\",
                \"fields\": [{
                  \"title\": \"Workflow\",
                  \"value\": \"${{ github.workflow }}\",
                  \"short\": true
                }, {
                  \"title\": \"Run Number\",
                  \"value\": \"${{ github.run_number }}\",
                  \"short\": true
                }, {
                  \"title\": \"Commit\",
                  \"value\": \"${{ github.sha }}\",
                  \"short\": true
                }, {
                  \"title\": \"View Run\",
                  \"value\": \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\",
                  \"short\": false
                }]
              }]
            }" || echo "Webhook notification failed"

