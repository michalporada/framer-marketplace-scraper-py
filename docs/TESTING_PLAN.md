# Plan TestÃ³w - Stabilizacja Scrapera

## ğŸ“‹ PrzeglÄ…d zmian do przetestowania

### Zadanie 1: UsuniÄ™cie bÅ‚Ä™dnej logiki sitemapa âœ…
- [ ] Test: Marketplace sitemap zwraca 5xx â†’ scraper koÅ„czy siÄ™ z exit code 2
- [ ] Test: Marketplace sitemap zwraca inne bÅ‚Ä™dy â†’ uÅ¼ywa cache (jeÅ›li dostÄ™pny)
- [ ] Test: Marketplace sitemap dziaÅ‚a â†’ zapisuje do cache

### Zadanie 2: Twarde zabezpieczenia na wyniki âœ…
- [ ] Test: < 50 URL-i z sitemapa â†’ scraper koÅ„czy siÄ™ bÅ‚Ä™dem
- [ ] Test: products_scraped == 0 â†’ CSV export zablokowany
- [ ] Test: products_scraped == 0 â†’ DB write zablokowany
- [ ] Test: Duplikaty wykryte â†’ DB write zablokowany dla duplikatÃ³w

### Zadanie 3: Naprawa problemÃ³w z wydÅ‚uÅ¼aniem czasu scrapÃ³w âœ…
- [ ] Test: Exponential backoff + jitter dziaÅ‚a (5 retry, max 5 min)
- [ ] Test: Timeout per request (25s) dziaÅ‚a
- [ ] Test: Globalny timeout (15 min) przerywa scraper
- [ ] Test: Slow requests (>10s) sÄ… logowane

### Zadanie 4: Blokowanie duplikatÃ³w âœ…
- [ ] Test: Deduplikacja po product ID dziaÅ‚a
- [ ] Test: Deduplikacja po product URL dziaÅ‚a
- [ ] Test: Deduplikacja po creator username dziaÅ‚a
- [ ] Test: Duplikaty sÄ… logowane

### Zadanie 5: Naprawa artefaktÃ³w GitHub Actions âœ…
- [ ] Test: Artifact upload dziaÅ‚a (scraped-data-latest)
- [ ] Test: Sync workflow pobiera artefakt poprawnie
- [ ] Test: Sprawdzenie artefaktu przed sync dziaÅ‚a

### Zadanie 6: Naprawa zmiennego czasu dziaÅ‚ania scraperÃ³w âœ…
- [ ] Test: Jitter cron dziaÅ‚a (0-60s delay)
- [ ] Test: Cache sitemapa zapisuje siÄ™ poprawnie
- [ ] Test: Cache sitemapa uÅ¼ywa siÄ™ przy bÅ‚Ä™dach non-5xx
- [ ] Test: Metrics.log zapisuje siÄ™ poprawnie

### Zadanie 7: StabilnoÅ›Ä‡ pipeline âœ…
- [ ] Test: Exit code 2 przy upstream 5xx
- [ ] Test: Exit code 1 przy innych bÅ‚Ä™dach
- [ ] Test: Webhook notification dziaÅ‚a (jeÅ›li skonfigurowany)

### Zadanie 8: Dodatkowe poprawki jakoÅ›ciowe âœ…
- [ ] Test: Weryfikacja sitemap parsowania (puste URL â†’ bÅ‚Ä…d)
- [ ] Test: Logowanie liczby rekordÃ³w przed zapisaniem

---

## ğŸ§ª SzczegÃ³Å‚owe testy

### Test 1: Sitemap - brak fallback, przerwanie przy 5xx

**Cel:** SprawdziÄ‡, Å¼e scraper nie uÅ¼ywa fallback do main sitemap i przerywa przy 5xx

**Kroki:**
1. Uruchom scraper lokalnie
2. SprawdÅº logi - powinien prÃ³bowaÄ‡ tylko marketplace sitemap
3. JeÅ›li marketplace sitemap zwraca 5xx â†’ scraper koÅ„czy siÄ™ z exit code 2

**Oczekiwany wynik:**
- Brak prÃ³b pobrania main sitemap
- Exit code 2 przy 5xx
- Log: "upstream_unavailable" lub "sitemap_fetch_failed_5xx"

**Komenda testowa:**
```bash
python -m src.main 10  # Test z limitem 10 produktÃ³w
```

---

### Test 2: Minimalny prÃ³g danych (50 URL-i)

**Cel:** SprawdziÄ‡, Å¼e scraper koÅ„czy siÄ™ bÅ‚Ä™dem, jeÅ›li < 50 URL-i

**Kroki:**
1. Tymczasowo ustaw `MIN_URLS_THRESHOLD=1000` w `.env`
2. Uruchom scraper
3. JeÅ›li sitemap zwraca < 1000 URL-i â†’ scraper powinien zakoÅ„czyÄ‡ siÄ™ bÅ‚Ä™dem

**Oczekiwany wynik:**
- Scraper koÅ„czy siÄ™ przed rozpoczÄ™ciem scrapowania produktÃ³w
- Log: "insufficient_urls" z found i required
- Exit code != 0

**Komenda testowa:**
```bash
# W .env ustaw: MIN_URLS_THRESHOLD=1000
python -m src.main
```

---

### Test 3: Blokowanie eksportu przy products_scraped == 0

**Cel:** SprawdziÄ‡, Å¼e CSV export i DB write sÄ… zablokowane przy braku produktÃ³w

**Kroki:**
1. Uruchom scraper, ktÃ³ry nie znajdzie Å¼adnych produktÃ³w (np. bÅ‚Ä™dny sitemap)
2. SprawdÅº logi - powinien byÄ‡ bÅ‚Ä…d "no_products_scraped"
3. SprawdÅº, Å¼e nie powstaÅ‚ plik CSV
4. SprawdÅº, Å¼e nie byÅ‚o zapisÃ³w do DB

**Oczekiwany wynik:**
- Log: "no_products_scraped" z message
- Log: "csv_export_blocked" z reason="no_products_scraped"
- Brak pliku CSV w `data/exports/`
- Brak zapisÃ³w do DB

**Komenda testowa:**
```bash
# Symulacja: ustaw bÅ‚Ä™dny sitemap URL w .env
# FRAMER_BASE_URL=https://invalid-url.com
python -m src.main
```

---

### Test 4: Deduplikacja produktÃ³w

**Cel:** SprawdziÄ‡, Å¼e duplikaty sÄ… wykrywane i blokowane

**Kroki:**
1. Uruchom scraper normalnie
2. SprawdÅº logi - powinny byÄ‡ warningi o duplikatach (jeÅ›li wystÄ™pujÄ…)
3. SprawdÅº, Å¼e duplikaty nie sÄ… zapisywane do DB
4. SprawdÅº, Å¼e liczba duplikatÃ³w jest logowana na koÅ„cu

**Oczekiwany wynik:**
- Log: "duplicate_product_id" lub "duplicate_product_url" dla kaÅ¼dego duplikatu
- Log: "db_write_skipped_duplicate" dla duplikatÃ³w
- Log: "duplicates_found" na koÅ„cu z count
- Brak duplikatÃ³w w DB

**Komenda testowa:**
```bash
python -m src.main 100  # Test z wiÄ™kszÄ… liczbÄ… produktÃ³w
```

---

### Test 5: Exponential backoff + jitter

**Cel:** SprawdziÄ‡, Å¼e retry uÅ¼ywa exponential backoff z jitter

**Kroki:**
1. Uruchom scraper z limitem
2. SprawdÅº logi retry - powinny pokazywaÄ‡ base_wait i jitter
3. SprawdÅº, Å¼e czasy oczekiwania rosnÄ… wykÅ‚adniczo (2s, 4s, 8s, 16s, 32s...)
4. SprawdÅº, Å¼e jitter jest dodawany (0-20% base wait)

**Oczekiwany wynik:**
- Log: "retry_attempt" z wait_time, base_wait, jitter
- Czas oczekiwania roÅ›nie wykÅ‚adniczo
- Jitter jest losowy (0-20% base wait)
- Max 5 retry

**Komenda testowa:**
```bash
# Uruchom z limitem i obserwuj logi
python -m src.main 50
# SprawdÅº logi w logs/scraper.log
```

---

### Test 6: Timeout per request (25s)

**Cel:** SprawdziÄ‡, Å¼e timeout per request dziaÅ‚a

**Kroki:**
1. Uruchom scraper
2. SprawdÅº, Å¼e requesty koÅ„czÄ… siÄ™ po 25s (jeÅ›li timeout)
3. SprawdÅº logi - powinny byÄ‡ timeout errors

**Oczekiwany wynik:**
- Requesty koÅ„czÄ… siÄ™ po ~25s przy timeout
- Log: "TimeoutError" lub podobny
- Retry po timeout

**Komenda testowa:**
```bash
# MoÅ¼na symulowaÄ‡ wolne requesty przez proxy lub rate limiting
python -m src.main 10
```

---

### Test 7: Globalny timeout (15 min)

**Cel:** SprawdziÄ‡, Å¼e globalny timeout przerywa scraper

**Kroki:**
1. Tymczasowo ustaw `GLOBAL_SCRAPING_TIMEOUT=60` (1 minuta) w `.env`
2. Uruchom scraper z duÅ¼Ä… liczbÄ… produktÃ³w
3. SprawdÅº, Å¼e scraper koÅ„czy siÄ™ po 1 minucie

**Oczekiwany wynik:**
- Scraper koÅ„czy siÄ™ po ~60s
- Log: "global_scraping_timeout" z elapsed i timeout
- Exit code != 0
- TimeoutError raised

**Komenda testowa:**
```bash
# W .env ustaw: GLOBAL_SCRAPING_TIMEOUT=60
python -m src.main 1000  # DuÅ¼a liczba produktÃ³w
```

---

### Test 8: Slow requests logging

**Cel:** SprawdziÄ‡, Å¼e wolne requesty (>10s) sÄ… logowane

**Kroki:**
1. Uruchom scraper
2. SprawdÅº logi - powinny byÄ‡ warningi dla requestÃ³w >10s

**Oczekiwany wynik:**
- Log: "slow_request" z url, duration, status_code
- Duration > 10.0s
- Message z threshold

**Komenda testowa:**
```bash
python -m src.main 50
# SprawdÅº logi w logs/scraper.log dla "slow_request"
```

---

### Test 9: Cache sitemapa

**Cel:** SprawdziÄ‡, Å¼e cache sitemapa dziaÅ‚a

**Kroki:**
1. Uruchom scraper normalnie (zapisze cache)
2. SprawdÅº, Å¼e plik `data/sitemap_cache.xml` zostaÅ‚ utworzony
3. Symuluj bÅ‚Ä…d non-5xx (np. 404) - scraper powinien uÅ¼yÄ‡ cache
4. SprawdÅº logi - powinien byÄ‡ "using_cached_sitemap"

**Oczekiwany wynik:**
- Plik `data/sitemap_cache.xml` istnieje po udanym scrapowaniu
- Cache jest uÅ¼ywany przy bÅ‚Ä™dach non-5xx
- Log: "sitemap_cache_saved" przy zapisie
- Log: "sitemap_cache_loaded" przy uÅ¼yciu
- Log: "using_cached_sitemap" przy uÅ¼yciu cache

**Komenda testowa:**
```bash
# Pierwszy run - zapisze cache
python -m src.main 10

# SprawdÅº cache
ls -lh data/sitemap_cache.xml

# Drugi run - uÅ¼yje cache jeÅ›li upstream nie dziaÅ‚a
python -m src.main 10
```

---

### Test 10: Metrics.log zapis

**Cel:** SprawdziÄ‡, Å¼e metryki sÄ… zapisywane do metrics.log

**Kroki:**
1. Uruchom scraper
2. SprawdÅº, Å¼e plik `data/metrics.log` zostaÅ‚ utworzony
3. SprawdÅº, Å¼e zawiera JSON Lines z metrykami

**Oczekiwany wynik:**
- Plik `data/metrics.log` istnieje
- Zawiera JSON Lines (jeden obiekt JSON per linia)
- KaÅ¼dy wpis ma timestamp i peÅ‚ne metryki
- Format: `{"timestamp": "...", "duration_seconds": ..., ...}`

**Komenda testowa:**
```bash
python -m src.main 10
cat data/metrics.log | jq .  # JeÅ›li masz jq
```

---

### Test 11: Deduplikacja przed eksportem CSV

**Cel:** SprawdziÄ‡, Å¼e CSV export deduplikuje produkty

**Kroki:**
1. UtwÃ³rz rÄ™cznie kilka zduplikowanych plikÃ³w JSON w `data/products/`
2. Uruchom export do CSV
3. SprawdÅº, Å¼e CSV nie zawiera duplikatÃ³w

**Oczekiwany wynik:**
- Log: "duplicates_removed_before_export" z count
- CSV zawiera tylko unikalne produkty
- Brak duplikatÃ³w w CSV

**Komenda testowa:**
```bash
# RÄ™cznie skopiuj plik produktu jako duplikat
cp data/products/templates/product1.json data/products/templates/product1_duplicate.json

# Export do CSV
python scripts/export_data.py

# SprawdÅº CSV - nie powinien zawieraÄ‡ duplikatÃ³w
```

---

### Test 12: Weryfikacja parsowania sitemapa

**Cel:** SprawdziÄ‡, Å¼e scraper koÅ„czy siÄ™ bÅ‚Ä™dem, jeÅ›li sitemap nie zawiera URL-i

**Kroki:**
1. Tymczasowo zmodyfikuj `parse_sitemap()` aby zwracaÅ‚a pusty dict
2. Uruchom scraper
3. SprawdÅº, Å¼e scraper koÅ„czy siÄ™ bÅ‚Ä™dem

**Oczekiwany wynik:**
- Log: "sitemap_parse_verification_failed"
- ValueError raised: "Sitemap parsing returned empty result"
- Exit code != 0

**Komenda testowa:**
```bash
# Test wymaga tymczasowej modyfikacji kodu
# Lub symulacji pustego sitemap
```

---

### Test 13: Logowanie liczby rekordÃ³w przed zapisaniem

**Cel:** SprawdziÄ‡, Å¼e liczba rekordÃ³w jest logowana przed eksportem

**Kroki:**
1. Uruchom scraper
2. SprawdÅº logi - powinien byÄ‡ log "records_before_export"

**Oczekiwany wynik:**
- Log: "records_before_export" z:
  - products_scraped
  - products_failed
  - creators_scraped
  - duplicates
  - total_products

**Komenda testowa:**
```bash
python -m src.main 50
# SprawdÅº logi w logs/scraper.log dla "records_before_export"
```

---

### Test 14: Jitter cron w GitHub Actions

**Cel:** SprawdziÄ‡, Å¼e jitter dziaÅ‚a w workflow

**Kroki:**
1. Uruchom workflow rÄ™cznie (workflow_dispatch)
2. SprawdÅº logi - powinien byÄ‡ log o jitterze

**Oczekiwany wynik:**
- Log: "â±ï¸ Adding Xs jitter before scraping..."
- X jest losowÄ… liczbÄ… 0-60
- Scraper startuje po jitterze

**Komenda testowa:**
```bash
# W GitHub Actions - workflow_dispatch
# SprawdÅº logi workflow
```

---

### Test 15: Exit codes

**Cel:** SprawdziÄ‡, Å¼e exit codes sÄ… poprawne

**Kroki:**
1. Uruchom scraper normalnie â†’ exit code 0
2. Uruchom scraper z bÅ‚Ä™dem upstream 5xx â†’ exit code 2
3. Uruchom scraper z innym bÅ‚Ä™dem â†’ exit code 1

**Oczekiwany wynik:**
- Success: exit code 0
- Upstream 5xx: exit code 2
- Inne bÅ‚Ä™dy: exit code 1

**Komenda testowa:**
```bash
# Normal run
python -m src.main 10
echo $?  # Powinno byÄ‡ 0

# Test upstream error (wymaga symulacji)
# SprawdÅº exit code
```

---

## ğŸš€ Szybki test end-to-end

**Kompleksowy test wszystkich funkcji:**

```bash
# 1. WyczyÅ›Ä‡ stare dane
rm -rf data/products/* data/creators/* data/exports/* data/*.log data/sitemap_cache.xml

# 2. Uruchom scraper z limitem
python -m src.main 20

# 3. SprawdÅº wyniki
echo "=== Sprawdzenie wynikÃ³w ==="
echo "Produkty:"
find data/products -name "*.json" | wc -l
echo "Kreatorzy:"
find data/creators -name "*.json" | wc -l
echo "Cache sitemapa:"
ls -lh data/sitemap_cache.xml 2>/dev/null || echo "Brak cache"
echo "Metryki:"
tail -1 data/metrics.log | jq . 2>/dev/null || tail -1 data/metrics.log
echo "Logi:"
grep -E "(insufficient_urls|no_products_scraped|duplicates_found|slow_request|global_scraping_timeout)" logs/*.log | tail -10

# 4. Test export CSV
python scripts/export_data.py
echo "CSV export:"
ls -lh data/exports/*.csv | tail -1

# 5. SprawdÅº exit code
echo "Exit code ostatniego runu: $?"
```

---

## ğŸ“Š Checklist przed commitem

- [ ] Wszystkie testy lokalne przeszÅ‚y
- [ ] Brak bÅ‚Ä™dÃ³w lintera
- [ ] Logi sÄ… czytelne i informatywne
- [ ] Exit codes sÄ… poprawne
- [ ] Cache sitemapa dziaÅ‚a
- [ ] Metrics.log zapisuje siÄ™ poprawnie
- [ ] Deduplikacja dziaÅ‚a
- [ ] Zabezpieczenia dziaÅ‚ajÄ… (min prÃ³g, blokowanie eksportu)
- [ ] Timeouty dziaÅ‚ajÄ… (per request i globalny)
- [ ] Retry z jitter dziaÅ‚a

---

## ğŸ” Monitoring po wdroÅ¼eniu

Po wdroÅ¼eniu zmian, monitoruj:

1. **Czas trwania runÃ³w** - sprawdÅº `data/metrics.log`
   - Czy czas jest stabilny?
   - Czy nie ma ekstremalnie dÅ‚ugich runÃ³w (>15 min)?

2. **Liczba duplikatÃ³w** - sprawdÅº logi
   - Czy sÄ… duplikaty?
   - JeÅ›li tak, dlaczego?

3. **Slow requests** - sprawdÅº logi
   - Ile jest slow requests?
   - KtÃ³re URL-e sÄ… wolne?

4. **Exit codes** - sprawdÅº GitHub Actions
   - Czy exit codes sÄ… poprawne?
   - Czy pipeline nie jest zielony przy bÅ‚Ä™dach?

5. **Cache sitemapa** - sprawdÅº uÅ¼ycie
   - Czy cache jest uÅ¼ywany?
   - Czy TTL jest odpowiedni?

---

## ğŸ“ Notatki testowe

**Data testÃ³w:** _______________

**Tester:** _______________

**Wyniki:**
- [ ] Test 1: Sitemap fallback - PASSED / FAILED
- [ ] Test 2: Minimalny prÃ³g - PASSED / FAILED
- [ ] Test 3: Blokowanie eksportu - PASSED / FAILED
- [ ] Test 4: Deduplikacja - PASSED / FAILED
- [ ] Test 5: Exponential backoff - PASSED / FAILED
- [ ] Test 6: Timeout per request - PASSED / FAILED
- [ ] Test 7: Globalny timeout - PASSED / FAILED
- [ ] Test 8: Slow requests - PASSED / FAILED
- [ ] Test 9: Cache sitemapa - PASSED / FAILED
- [ ] Test 10: Metrics.log - PASSED / FAILED
- [ ] Test 11: Deduplikacja CSV - PASSED / FAILED
- [ ] Test 12: Weryfikacja sitemap - PASSED / FAILED
- [ ] Test 13: Logowanie rekordÃ³w - PASSED / FAILED
- [ ] Test 14: Jitter cron - PASSED / FAILED
- [ ] Test 15: Exit codes - PASSED / FAILED

**Uwagi:**
_________________________________________________
_________________________________________________

