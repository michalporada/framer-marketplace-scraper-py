# Propozycja Architektury Projektu - Scraper Framer Marketplace

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ scrape.yml              # GitHub Actions - scheduled scraping
â”‚       â”œâ”€â”€ ci.yml                   # GitHub Actions - CI/CD
â”‚       â””â”€â”€ backup.yml               # GitHub Actions - backup danych
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ marketplace_scraper.py  # GÅ‚Ã³wny scraper marketplace
â”‚   â”‚   â”œâ”€â”€ product_scraper.py       # Scraper pojedynczego produktu
â”‚   â”‚   â”œâ”€â”€ creator_scraper.py       # Scraper profilu twÃ³rcy
â”‚   â”‚   â”œâ”€â”€ category_scraper.py      # Scraper kategorii (opcjonalnie)
â”‚   â”‚   â””â”€â”€ sitemap_scraper.py       # Scraper sitemap.xml
â”‚   â”‚
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ product_parser.py       # Parsowanie danych produktu
â”‚   â”‚   â”œâ”€â”€ creator_parser.py       # Parsowanie danych twÃ³rcy
â”‚   â”‚   â”œâ”€â”€ category_parser.py       # Parsowanie danych kategorii (opcjonalnie)
â”‚   â”‚   â””â”€â”€ review_parser.py        # Parsowanie recenzji
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ product.py              # Model Pydantic produktu
â”‚   â”‚   â”œâ”€â”€ creator.py              # Model Pydantic twÃ³rcy
â”‚   â”‚   â”œâ”€â”€ category.py             # Model Pydantic kategorii (opcjonalnie)
â”‚   â”‚   â””â”€â”€ review.py               # Model Pydantic recenzji
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py             # PoÅ‚Ä…czenie z bazÄ… danych
â”‚   â”‚   â”œâ”€â”€ file_storage.py         # Zapis do plikÃ³w (JSON, CSV)
â”‚   â”‚   â””â”€â”€ backup.py               # Backup danych
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py        # Ograniczenie czÄ™stotliwoÅ›ci requestÃ³w
â”‚   â”‚   â”œâ”€â”€ user_agents.py          # Rotacja User-Agent
â”‚   â”‚   â”œâ”€â”€ logger.py               # Konfiguracja logowania
â”‚   â”‚   â”œâ”€â”€ retry.py                # Retry logic
â”‚   â”‚   â”œâ”€â”€ validators.py           # Walidacja danych
â”‚   â”‚   â””â”€â”€ normalizers.py          # Normalizacja dat i statystyk (Opcja B) â­
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py             # Konfiguracja (pydantic-settings)
â”‚   â”‚
â”‚   â””â”€â”€ main.py                     # Entry point aplikacji
â”‚
â”œâ”€â”€ api/                            # (Opcjonalnie) API endpoints
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                     # FastAPI app
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ products.py
â”‚   â”‚   â”œâ”€â”€ creators.py
â”‚   â”‚   â””â”€â”€ reviews.py
â”‚   â””â”€â”€ dependencies.py
â”‚
â”œâ”€â”€ frontend/                       # (Opcjonalnie) Dashboard
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/                   # Zapisane dane produktÃ³w (JSON)
â”‚   â”‚   â”œâ”€â”€ templates/              # Szablony ({product_id}.json)
â”‚   â”‚   â”œâ”€â”€ components/             # Komponenty ({product_id}.json)
â”‚   â”‚   â”œâ”€â”€ vectors/                # Wektory ({product_id}.json)
â”‚   â”‚   â””â”€â”€ plugins/                 # Wtyczki ({product_id}.json) â­
â”‚   â”œâ”€â”€ creators/                   # Profile twÃ³rcÃ³w jako osobne pliki JSON ({username}.json)
â”‚   â”œâ”€â”€ categories/                 # Zapisane dane kategorii (JSON) (opcjonalnie)
â”‚   â”œâ”€â”€ exports/                    # Eksporty CSV
â”‚   â””â”€â”€ images/                     # Pobrane obrazy (opcjonalnie)
â”‚
â”œâ”€â”€ logs/                           # Logi scrapera
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scrapers/
â”‚   â”œâ”€â”€ test_parsers/
â”‚   â”œâ”€â”€ test_models/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py                 # Setup bazy danych
â”‚   â””â”€â”€ export_data.py              # Export danych do CSV
â”‚   # clean_data.py - nie zaimplementowane (opcjonalne)
â”‚
â”‚   # docs/ - folder nie istnieje (dokumentacja w gÅ‚Ã³wnym katalogu i README.md)
â”‚
â”œâ”€â”€ .env.example                    # PrzykÅ‚adowe zmienne Å›rodowiskowe
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml          # Pre-commit hooks
â”œâ”€â”€ pyproject.toml                   # Python project config (poetry/pip)
â”œâ”€â”€ requirements.txt                 # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ requirements-dev.txt             # ZaleÅ¼noÅ›ci dev
â”œâ”€â”€ README.md
â”œâ”€â”€ STACK_TECHNICZNY.md              # Dokument o stacku
â””â”€â”€ PROPOZYCJA_ARCHITEKTURY.md      # Ten dokument
```

## ğŸ”§ Komponenty Systemu

### 1. Scrapers (`src/scrapers/`)

#### `marketplace_scraper.py`
- GÅ‚Ã³wny orchestrator scrapowania
- Koordynuje pracÄ™ innych scraperÃ³w
- ZarzÄ…dza kolejkÄ… URL-i do przetworzenia
- ObsÅ‚uguje resume capability

#### `sitemap_scraper.py`
- Pobiera i parsuje `sitemap.xml` (marketplace lub gÅ‚Ã³wny sitemap)
- WyodrÄ™bnia wszystkie URL-e:
  - **Produkty**: templates, components, vectors, **plugins** â­
  - **Kategorie**: `/marketplace/category/{nazwa}/`
  - **Profile**: `/@{username}/` (wszystko zaczynajÄ…ce siÄ™ od `@`)
  - **Strony pomocowe**: `/help/articles/...marketplace...`
- Filtruje wedÅ‚ug typu produktu
- UÅ¼ywa tylko marketplace sitemap (brak fallback do gÅ‚Ã³wnego sitemap)
- Cache sitemapa jako fallback przy bÅ‚Ä™dach non-5xx (TTL: 1h)
- Przerwanie scrapera przy bÅ‚Ä™dach 5xx (exit code 2)

#### `product_scraper.py`
- Scrapuje pojedynczy produkt (template/component/vector/**plugin**)
- ObsÅ‚uguje wszystkie typy produktÃ³w:
  - Templates: `/marketplace/templates/{nazwa}/`
  - Components: `/marketplace/components/{nazwa}/`
  - Vectors: `/marketplace/vectors/{nazwa}/`
  - **Plugins**: `/marketplace/plugins/{nazwa}/` â­
- Pobiera stronÄ™ produktu
- WywoÅ‚uje parsery do ekstrakcji danych

#### `creator_scraper.py`
- Scrapuje profil twÃ³rcy (`/@username/`)
- **UWAGA**: Username moÅ¼e zawieraÄ‡ znaki specjalne (np. `/@-790ivi/`)
- ObsÅ‚uguje wszystkie profile zaczynajÄ…ce siÄ™ od `@`
- Pobiera statystyki twÃ³rcy
- Zbiera informacje o wszystkich produktach twÃ³rcy (templates/components/vectors/plugins)

### 2. Parsers (`src/parsers/`)

#### `product_parser.py`
- Parsuje HTML strony produktu
- ObsÅ‚uguje wszystkie typy produktÃ³w (templates/components/vectors/**plugins**)
- Ekstrahuje: nazwa, cena, opis, funkcje, obrazy, recenzje, typ produktu
- UÅ¼ywa selektorÃ³w CSS z dokumentacji
- Identyfikuje typ produktu z URL lub HTML
- **Components Installs**: WyciÄ…gane z JSON danych Next.js (priorytet) lub z HTML tekstu. MoÅ¼e byÄ‡ niedostÄ™pne dla niektÃ³rych komponentÃ³w.

#### `creator_parser.py`
- Parsuje profil twÃ³rcy
- Ekstrahuje: statystyki, produkty, bio, social media, avatar
- Profil jest zapisywany jako osobny plik JSON: `data/creators/{username}.json`
- **Avatar**: WyciÄ…gany z JSON danych Next.js (priorytet), pomijane placeholdery API
- **Social Media**: WyciÄ…gane z JSON danych Next.js, automatycznie filtrowane linki Framer. ObsÅ‚ugiwane: Twitter/X, LinkedIn, Instagram, GitHub, Dribbble, Behance, YouTube

#### `review_parser.py`
- Parsuje recenzje produktu
- Ekstrahuje: ocena, treÅ›Ä‡, autor, data

### 3. Models (`src/models/`)

#### Pydantic Models

**Product Model (Opcja B - Normalizacja):**
- **NormalizedDate:** Format daty z surowym i znormalizowanym formatem
  - `raw`: Format surowy z HTML (np. "5 months ago", "3mo ago")
  - `normalized`: ISO 8601 (np. "2024-10-15T00:00:00Z")
- **NormalizedStatistic:** Format statystyki z surowym i znormalizowanym formatem
  - `raw`: Format surowy z HTML (np. "19.8K Views", "1,200 Vectors")
  - `normalized`: Liczba caÅ‚kowita (np. 19800, 1200)
- **ProductStats:** Statystyki produktu (rÃ³Å¼ne dla rÃ³Å¼nych typÃ³w)
  - `views`, `pages`, `users`, `installs`, `vectors` (opcjonalnie)
  - Wszystkie jako `NormalizedStatistic`
- **ProductMetadata:** Metadane produktu
  - `published_date`, `last_updated` jako `NormalizedDate`
  - `version` (string, dla plugins)
- **Product:** GÅ‚Ã³wny model produktu
  - Typ produktu: `template`, `component`, `vector`, **`plugin`** â­
  - ObsÅ‚uga wszystkich pÃ³l z dokumentacji
  - Wszystkie daty i statystyki w formacie Opcji B

**Creator Model:**
- **Creator:** Walidacja danych twÃ³rcy
  - Username moÅ¼e zawieraÄ‡ znaki specjalne (np. `/@-790ivi/`)
  - Lista produktÃ³w (templates/components/vectors/plugins)

**Category Model (opcjonalnie):**
- **Category:** Walidacja danych kategorii
  - Nazwa, URL, opis, lista produktÃ³w

**Review Model:**
- **Review:** Walidacja recenzji
  - Ocena, treÅ›Ä‡, autor, data

**Automatyczna serializacja:** Wszystkie modele automatycznie serializujÄ… do JSON

### 4. Storage (`src/storage/`)

#### `file_storage.py`
- Zapis produktÃ³w do JSON (jeden plik per produkt: `products/{type}/{product_id}.json`)
- Zapis kreatorÃ³w do JSON (jeden plik per kreator: `creators/{username}.json`)
- Eksport produktÃ³w do CSV (`export_products_to_csv()`)
- Eksport kreatorÃ³w do CSV (`export_creators_to_csv()`)
- Incremental saves (zapis przyrostowy)

#### `database.py`
- PoÅ‚Ä…czenie z PostgreSQL/MongoDB
- Zapis danych przez SQLAlchemy/ORM
- Migracje schematu

#### `backup.py`
- Backup danych do GitHub Releases
- Backup do cloud storage (S3, etc.)

### 5. Utils (`src/utils/`)

#### `rate_limiter.py`
- Ograniczenie do 1-2 req/s
- Randomizacja opÃ³ÅºnieÅ„
- Respektowanie robots.txt

#### `retry.py`
- Exponential backoff
- ObsÅ‚uga timeoutÃ³w
- Retry logic dla failed requests

#### `logger.py`
- Strukturalne logowanie (structlog)
- RÃ³Å¼ne poziomy logowania
- Rotacja logÃ³w

#### `normalizers.py` â­
- **Normalizacja dat** (Opcja B):
  - `parse_relative_date()`: Konwertuje "X months ago" â†’ ISO 8601
  - ObsÅ‚uguje formaty: "X months ago", "Xmo ago", "Xw ago", "X days ago"
  - Zwraca: `{"raw": "...", "normalized": "ISO 8601"}`
- **Normalizacja statystyk** (Opcja B):
  - `parse_statistic()`: Konwertuje "19.8K Views" â†’ 19800
  - ObsÅ‚uguje formaty: "X.XK", "XK", "X,XXX", "XXX"
  - Zwraca: `{"raw": "...", "normalized": int}`
- **UÅ¼ycie:** Parser wywoÅ‚uje normalizatory przed zapisem do modelu

## ğŸ”„ Flow Scrapowania

```
1. START
   â”‚
   â”œâ”€â–¶ main.py (entry point)
   â”‚
2. INITIALIZATION
   â”œâ”€â–¶ Wczytaj konfiguracjÄ™ (.env)
   â”œâ”€â–¶ SprawdÅº robots.txt
   â”œâ”€â–¶ Inicjalizuj logger
   â””â”€â–¶ Przygotuj sesjÄ™ HTTP
   â”‚
3. GET PRODUCT LIST
   â”œâ”€â–¶ OPCJA A: Sitemap (REKOMENDOWANE) â­
   â”‚   â”œâ”€â–¶ sitemap_scraper.py â†’ pobierz sitemap.xml
   â”‚   â”‚   â”œâ”€â–¶ SprÃ³buj: `/marketplace/sitemap.xml` (tylko marketplace sitemap)
   â”‚   â”‚   â”œâ”€â–¶ JeÅ›li 5xx: przerwij scraper natychmiast (exit code 2)
   â”‚   â”‚   â”œâ”€â–¶ JeÅ›li inne bÅ‚Ä™dy: uÅ¼yj cache sitemapa (jeÅ›li dostÄ™pny, TTL: 1h)
   â”‚   â”‚   â””â”€â–¶ Weryfikacja: scraper koÅ„czy siÄ™ bÅ‚Ä™dem, jeÅ›li sitemap nie zawiera URL-i
   â”‚   â”œâ”€â–¶ WyodrÄ™bnij wszystkie URL-e:
   â”‚   â”‚   â”œâ”€â–¶ Produkty:
   â”‚   â”‚   â”‚   â”œâ”€â–¶ Templates: `/marketplace/templates/{nazwa}/`
   â”‚   â”‚   â”‚   â”œâ”€â–¶ Components: `/marketplace/components/{nazwa}/`
   â”‚   â”‚   â”‚   â”œâ”€â–¶ Vectors: `/marketplace/vectors/{nazwa}/`
   â”‚   â”‚   â”‚   â””â”€â–¶ Plugins: `/marketplace/plugins/{nazwa}/` â­
   â”‚   â”‚   â”œâ”€â–¶ Kategorie: `/marketplace/category/{nazwa}/`
   â”‚   â”‚   â”œâ”€â–¶ Profile: `/@{username}/` (wszystko z `@`)
   â”‚   â”‚   â””â”€â–¶ Strony pomocowe: `/help/articles/...marketplace...`
   â”‚   â””â”€â–¶ Filtruj wedÅ‚ug typu (templates/components/vectors/plugins)
   â”‚
   â””â”€â–¶ OPCJA B: Scraping listy
       â”œâ”€â–¶ marketplace_scraper.py â†’ pobierz /marketplace
       â””â”€â–¶ Parsuj karty produktÃ³w
   â”‚
4. SCRAPE PRODUCTS
   â”œâ”€â–¶ Dla kaÅ¼dego produktu (rÃ³wnolegle z limitem):
   â”‚   â”œâ”€â–¶ product_scraper.py â†’ pobierz stronÄ™ produktu
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje: templates/components/vectors/**plugins** â­
   â”‚   â”œâ”€â–¶ product_parser.py â†’ ekstrahuj dane
   â”‚   â”‚   â””â”€â–¶ Identyfikuj typ produktu (template/component/vector/plugin)
   â”‚   â”œâ”€â–¶ creator_scraper.py â†’ pobierz profil twÃ³rcy (`/@username/`)
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje username z znakami specjalnymi
   â”‚   â”œâ”€â–¶ creator_parser.py â†’ ekstrahuj dane twÃ³rcy
   â”‚   â”œâ”€â–¶ save_creator_json() â†’ zapisz profil twÃ³rcy jako osobny plik (data/creators/{username}.json)
   â”‚   â”œâ”€â–¶ review_parser.py â†’ ekstrahuj recenzje
   â”‚   â”œâ”€â–¶ Walidacja danych (Pydantic)
   â”‚   â””â”€â–¶ Zapis danych (file_storage.py lub database.py)
   â”‚       â”œâ”€â–¶ Zapis produktu: products/{type}/{product_id}.json
   â”‚       â””â”€â–¶ Zapis kreatora: creators/{username}.json (osobny plik)
   â”‚
4b. SCRAPE CATEGORIES (opcjonalnie)
   â”œâ”€â–¶ Dla kaÅ¼dej kategorii z sitemap:
   â”‚   â”œâ”€â–¶ category_scraper.py â†’ pobierz `/marketplace/category/{nazwa}/`
   â”‚   â”œâ”€â–¶ category_parser.py â†’ ekstrahuj:
   â”‚   â”‚   â”œâ”€â–¶ Nazwa kategorii
   â”‚   â”‚   â”œâ”€â–¶ Opis kategorii
   â”‚   â”‚   â”œâ”€â–¶ Lista produktÃ³w w kategorii
   â”‚   â”‚   â””â”€â–¶ Liczba produktÃ³w
   â”‚   â””â”€â–¶ Zapis danych kategorii
   â”‚
4c. SCRAPE PROFILES (opcjonalnie)
   â”œâ”€â–¶ Dla kaÅ¼dego profilu z sitemap (`/@username/`):
   â”‚   â”œâ”€â–¶ creator_scraper.py â†’ pobierz profil
   â”‚   â”‚   â””â”€â–¶ ObsÅ‚uguje username z znakami specjalnymi (np. `/@-790ivi/`)
   â”‚   â”œâ”€â–¶ creator_parser.py â†’ ekstrahuj:
   â”‚   â”‚   â”œâ”€â–¶ Username (z URL)
   â”‚   â”‚   â”œâ”€â–¶ Nazwa wyÅ›wietlana
   â”‚   â”‚   â”œâ”€â–¶ Bio/opis
   â”‚   â”‚   â”œâ”€â–¶ Avatar
   â”‚   â”‚   â”œâ”€â–¶ Lista produktÃ³w (templates/components/vectors/plugins)
   â”‚   â”‚   â”œâ”€â–¶ Statystyki
   â”‚   â”‚   â””â”€â–¶ Linki do social media
   â”‚   â””â”€â–¶ Zapis danych profilu
   â”‚
5. POST-PROCESSING
   â”œâ”€â–¶ Czyszczenie danych
   â”œâ”€â–¶ Normalizacja danych (Opcja B) â­
   â”‚   â”œâ”€â–¶ normalizers.py â†’ parse_relative_date() dla dat
   â”‚   â””â”€â–¶ normalizers.py â†’ parse_statistic() dla statystyk
   â”œâ”€â–¶ Weryfikacja kompletnoÅ›ci
   â”œâ”€â–¶ Dekodowanie URL-i obrazÃ³w (Next.js Image â†’ oryginalny URL)
   â””â”€â–¶ Generowanie raportÃ³w
   â”‚
6. SAVE & BACKUP
   â”œâ”€â–¶ Zapis do JSON/CSV
   â”œâ”€â–¶ Zapis do bazy danych (opcjonalnie)
   â””â”€â–¶ Backup (GitHub Actions artifacts)
   â”‚
7. END
```

## ğŸš€ Deployment Strategy

### GitHub Actions Workflows

#### 1. `.github/workflows/scrape.yml`
```yaml
name: Daily Scrape
on:
  schedule:
    - cron: '0 2 * * *'  # Codziennie o 2:00 UTC
  workflow_dispatch:     # RÄ™czne uruchomienie

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python src/main.py
      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: data/
```

#### 2. `.github/workflows/ci.yml`
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - name: Install dependencies
        run: pip install -r requirements-dev.txt
      - name: Run tests
        run: pytest
      - name: Lint
        run: ruff check .
      - name: Type check
        run: mypy src/
```

### Vercel Deployment (Opcjonalnie)

#### Struktura dla API:
```
api/
â”œâ”€â”€ main.py              # FastAPI app
â”œâ”€â”€ vercel.json          # Vercel config
â””â”€â”€ routes/
```

#### `vercel.json`:
```json
{
  "builds": [
    {
      "src": "api/main.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "api/main.py"
    }
  ]
}
```

## ğŸ“Š Monitoring & Logging

### Logging Structure
```python
# PrzykÅ‚ad uÅ¼ycia structlog
logger.info("scraping_started", product_count=1000)
logger.info("product_scraped", product_id="123", status="success")
logger.error("scraping_failed", error="timeout", retry_count=2)
```

### Metrics to Track
- Liczba przetworzonych produktÃ³w
- Liczba bÅ‚Ä™dÃ³w
- Czas scrapowania
- Success rate
- Rate limit violations

### Notifications
- GitHub Actions: email/Slack o statusie
- Error tracking: Sentry (opcjonalnie)

## ğŸ” Security & Best Practices

### Environment Variables
```bash
# .env.example
DATABASE_URL=postgresql://...
FRAMER_BASE_URL=https://www.framer.com
MARKETPLACE_SITEMAP_URL=https://www.framer.com/marketplace/sitemap.xml
# MAIN_SITEMAP_URL - NIE UÅ»YWANY (brak fallback do gÅ‚Ã³wnego sitemap)
# Cache sitemapa uÅ¼ywany jako fallback przy bÅ‚Ä™dach non-5xx
RATE_LIMIT=1.0
MAX_RETRIES=5  # 5 retry z exponential backoff + jitter (max 5 min)
TIMEOUT=25  # Timeout per request (20-30s)
GLOBAL_SCRAPING_TIMEOUT=900  # Globalny timeout na caÅ‚y scraping (15 min)
LOG_LEVEL=INFO
MIN_URLS_THRESHOLD=50  # Minimalny prÃ³g URL-i z sitemapa
# Opcjonalne - typy produktÃ³w do scrapowania
SCRAPE_TEMPLATES=true
SCRAPE_COMPONENTS=true
SCRAPE_VECTORS=true
SCRAPE_PLUGINS=true  # â­
SCRAPE_CATEGORIES=false  # Opcjonalnie
SCRAPE_PROFILES=false  # Opcjonalnie
```

### Rate Limiting
- 1-2 requestÃ³w na sekundÄ™
- Randomizacja opÃ³ÅºnieÅ„ (0.5-2s)
- Respektowanie robots.txt

### Error Handling
- Retry z exponential backoff
- Timeout handling
- Graceful degradation
- Checkpoint system (resume capability)

### Data Validation
- Pydantic models dla wszystkich danych
- Walidacja przed zapisem
- Sprawdzanie wymaganych pÃ³l

### Data Normalization (Opcja B) â­
- **Normalizacja dat**: Relatywne daty ("X months ago") â†’ ISO 8601
  - Format: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
  - Funkcja: `utils/normalizers.py::parse_relative_date()`
- **Normalizacja statystyk**: SkrÃ³cone formaty ("19.8K") â†’ liczby caÅ‚kowite
  - Format: `{"raw": "19.8K Views", "normalized": 19800}`
  - Funkcja: `utils/normalizers.py::parse_statistic()`
- **Zapis obu formatÃ³w**: Zapewnia weryfikacjÄ™ i elastycznoÅ›Ä‡ analizy
- **Modele Pydantic**: `NormalizedDate`, `NormalizedStatistic` w modelach produktu

## ğŸ¯ Next Steps

### Faza 1: Setup (MVP)
1. âœ… StwÃ³rz strukturÄ™ projektu
2. âœ… Setup Python environment (poetry/pip)
3. âœ… Implementuj podstawowy scraper (sitemap â†’ products)
4. âœ… Implementuj rate limiting i error handling
5. âœ… Implementuj normalizacjÄ™ danych (Opcja B) â­
   - `utils/normalizers.py` z funkcjami parse_relative_date() i parse_statistic()
   - Modele Pydantic z NormalizedDate i NormalizedStatistic
6. âœ… Test na maÅ‚ej prÃ³bce (10-20 produktÃ³w)

### Faza 2: Rozszerzenie
1. â¬œ Dodaj scraping wszystkich typÃ³w produktÃ³w (templates/components/vectors/**plugins**)
2. â¬œ Dodaj scraping twÃ³rcÃ³w i recenzji
3. â¬œ Dodaj scraping kategorii (opcjonalnie)
4. â¬œ Implementuj storage (database)
5. â¬œ Setup GitHub Actions
6. â¬œ Dodaj monitoring i notyfikacje

### Faza 3: Production
1. â¬œ API endpoints (FastAPI/Vercel)
2. â¬œ Dashboard (Next.js/Vercel)
3. â¬œ Production database
4. â¬œ Error tracking (Sentry)

---

*Dokument wygenerowany na podstawie REKOMENDACJE_SCRAPERA_FRAMER.md*

