# Scraper Framer Marketplace V2

Zaawansowany scraper do zbierania danych z Framer Marketplace, umoÅ¼liwiajÄ…cy automatyzacjÄ™ pobierania informacji o:

- **Produktach**: Szablony (templates), Komponenty (components), Wektory (vectors), **Wtyczki (plugins)** â­
- **TwÃ³rcach/UÅ¼ytkownikach**: Profile z username (moÅ¼e zawieraÄ‡ znaki specjalne)
- **Kategoriach**: Kategorie produktÃ³w w marketplace

## ğŸ“š Dokumentacja

### GÅ‚Ã³wne Dokumenty

1. **[DEPLOYMENT_PLAN.md](./docs/DEPLOYMENT_PLAN.md)** - Kompletny plan wdroÅ¼enia (Vercel + Railway + Supabase):
   - Krok po kroku instrukcje deploymentu
   - Konfiguracja wszystkich serwisÃ³w
   - Troubleshooting i best practices

2. **[STACK_TECHNICZNY.md](./documentation_sources/STACK_TECHNICZNY.md)** - SzczegÃ³Å‚owy opis stacku technicznego, w tym:
   - Biblioteki Python i narzÄ™dzia
   - Opcje baz danych
   - GitHub Actions i Vercel
   - Rekomendacje deployment

3. **[PROPOZYCJA_ARCHITEKTURY.md](./documentation_sources/PROPOZYCJA_ARCHITEKTURY.md)** - Propozycja struktury projektu:
   - Struktura folderÃ³w
   - Opis komponentÃ³w
   - Flow scrapowania
   - Deployment strategy

4. **[REKOMENDACJE_SCRAPERA_FRAMER.md](./documentation_sources/REKOMENDACJE_SCRAPERA_FRAMER.md)** - SzczegÃ³Å‚owa analiza Framer Marketplace:
   - Analiza techniczna strony
   - Struktura URL-i i selektory CSS
   - Zalecane dane do zbierania
   - Uwagi techniczne

## ğŸš€ Quick Start

### 1. Instalacja

```bash
# Sklonuj repozytorium
git clone <repo-url>
cd scraper-v2

# UtwÃ³rz Å›rodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt
```

### 2. Konfiguracja

```bash
# Skopiuj przykÅ‚adowy plik .env
cp .env.example .env

# Edytuj .env z odpowiednimi wartoÅ›ciami
# WiÄ™kszoÅ›Ä‡ wartoÅ›ci ma sensowne domyÅ›lne ustawienia
```

GÅ‚Ã³wne zmienne Å›rodowiskowe:
- `FRAMER_BASE_URL` - URL do Framer (domyÅ›lnie: https://www.framer.com)
- `RATE_LIMIT` - Limit requestÃ³w na sekundÄ™ (domyÅ›lnie: 1.0)
- `MAX_RETRIES` - Maksymalna liczba ponownych prÃ³b (domyÅ›lnie: 3)
- `LOG_LEVEL` - Poziom logowania (INFO, DEBUG, WARNING, ERROR)
- `CHECKPOINT_ENABLED` - WÅ‚Ä…cz checkpoint system (domyÅ›lnie: true)
- `SCRAPE_TEMPLATES`, `SCRAPE_COMPONENTS`, `SCRAPE_VECTORS`, `SCRAPE_PLUGINS` - Typy produktÃ³w do scrapowania

### 3. Uruchomienie

```bash
# Podstawowe uruchomienie (scrapuje wszystkie produkty)
python3 -m src.main

# Ograniczenie liczby produktÃ³w (np. 10 dla testÃ³w)
python3 -m src.main 10

# Scrapowanie tylko okreÅ›lonych typÃ³w produktÃ³w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki

# Scrapowanie tylko kreatorÃ³w
python3 -m src.main --creators-only        # Wszyscy kreatorzy
python3 -m src.main --creators-only 10     # Z limitem
python3 -m src.main -c 10                  # KrÃ³tka wersja

# Scrapowanie tylko kategorii
python3 -m src.main --categories-only       # Wszystkie kategorie
python3 -m src.main --categories-only 10   # Z limitem
python3 -m src.main -cat 10                # KrÃ³tka wersja

# Export danych do CSV
python scripts/export_data.py

# Export tylko okreÅ›lonego typu produktu
python scripts/export_data.py --type template

# Setup bazy danych (opcjonalnie)
python scripts/setup_db.py --db-type postgresql
```

### 4. GitHub Actions (Automatyzacja)

Scraper moÅ¼e byÄ‡ uruchamiany automatycznie przez GitHub Actions:

- **Scheduled**: Codziennie o 2:00 UTC (zobacz `.github/workflows/scrape.yml`)
- **Manual**: RÄ™czne uruchomienie przez `workflow_dispatch`

**Zachowanie historii:**
- Najnowsze dane: `data/` (nadpisywane przy kaÅ¼dym scrapie)
- Archiwum z datÄ…: `scraped-data-YYYY-MM-DD/` (zachowuje historiÄ™)
- Artifacts w GitHub: 90 dni przechowywania

**Automatyczne porÃ³wnywanie:**
Po kilku dniach scrapowania moÅ¼esz porÃ³wnywaÄ‡ zmiany w czasie przez API (zobacz sekcjÄ™ API poniÅ¼ej).

## ğŸ› ï¸ Stack Techniczny

### Backend
- **Python 3.11+** - jÄ™zyk gÅ‚Ã³wny
- **httpx** - async HTTP client
- **BeautifulSoup4** - parsowanie HTML
- **pydantic v2** - walidacja danych z normalizacjÄ… (Opcja B)
- **pandas** - manipulacja danych i eksport do CSV
- **SQLAlchemy** - ORM (opcjonalnie, dla PostgreSQL)

### NarzÄ™dzia
- **structlog** - strukturalne logowanie
- **tenacity** - retry logic z exponential backoff
- **fake-useragent** - rotacja User-Agent headers
- **tqdm** - progress bars

### Deployment & Automation
- **GitHub Actions** - automatyczne scrapowanie (scheduled)
- **Railway** - FastAPI backend (https://framer-marketplace-scraper-py-production.up.railway.app)
- **Vercel** - Next.js frontend (https://framer-marketplace-scraper-py.vercel.app)
- **Supabase** - PostgreSQL database (skonfigurowana)

### Storage
- **JSON/CSV** - podstawowe (zalecane na start)
- **PostgreSQL** - dla wiÄ™kszych projektÃ³w
- **GitHub Artifacts** - backup danych

## ğŸ“‹ FunkcjonalnoÅ›ci

### âœ… Zaimplementowane

- [x] Scrapowanie produktÃ³w z sitemap.xml (templates/components/vectors/**plugins**)
- [x] Scrapowanie danych twÃ³rcÃ³w (profile z `@username`)
- [x] Zapisywanie profilÃ³w twÃ³rcÃ³w jako osobne pliki JSON (`data/creators/{username}.json`)
- [x] Scrapowanie kategorii (opcjonalnie)
- [x] Rate limiting i error handling
- [x] Zapis do JSON/CSV (organizacja wedÅ‚ug typu produktu)
- [x] Eksport kreatorÃ³w do CSV (`export_creators_to_csv()`)
- [x] Automatyzacja przez GitHub Actions
- [x] Resume capability (wznowienie po przerwie) - checkpoint system
- [x] Walidacja danych (Pydantic)
- [x] Monitoring i logowanie (structlog)
- [x] Normalizacja danych (Opcja B - raw + normalized)
- [x] ObsÅ‚uga rÃ³Å¼nych typÃ³w produktÃ³w (rÃ³Å¼ne statystyki i pola)

### âœ… API Endpoints (FastAPI)

API jest dostÄ™pne i gotowe do uÅ¼ycia:

**PorÃ³wnywanie produktÃ³w w czasie:**
```bash
GET /api/products/{product_id}/changes
```
PorÃ³wnuje dane produktu miÄ™dzy rÃ³Å¼nymi scrapami, wykrywa zmiany w statystykach, cenie i metadanych.

**PorÃ³wnywanie kategorii:**
```bash
GET /api/products/categories/comparison
GET /api/products/categories/comparison?product_type=template
GET /api/products/categories/comparison?category=Agency
```
PorÃ³wnuje Å‚Ä…cznÄ… liczbÄ™ wyÅ›wietleÅ„ kategorii miÄ™dzy scrapami z procentowym wzrostem/spadkiem.

**Inne endpointy:**
- `GET /api/products` - lista produktÃ³w
- `GET /api/products/{id}` - pojedynczy produkt
- `GET /api/creators` - lista twÃ³rcÃ³w
- `GET /api/creators/{username}` - pojedynczy twÃ³rca

### ğŸ”® Opcjonalne (Faza 2+)

- [ ] Dashboard (Next.js)
- [ ] Baza danych (PostgreSQL) - setup script gotowy
- [ ] Error tracking (Sentry)
- [ ] Notyfikacje (Slack/Email)

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/          # Scrapery (sitemap, product, creator, category)
â”‚   â”œâ”€â”€ parsers/           # Parsery HTML (product, creator, category)
â”‚   â”œâ”€â”€ models/            # Modele Pydantic (Product, Creator, Category)
â”‚   â”œâ”€â”€ storage/           # Zapis danych (file_storage, database)
â”‚   â”œâ”€â”€ utils/             # NarzÄ™dzia (logger, rate_limiter, retry, normalizers, checkpoint)
â”‚   â”œâ”€â”€ config/            # Konfiguracja (settings)
â”‚   â””â”€â”€ main.py            # Entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/          # Zapisane produkty (templates/, components/, vectors/, plugins/)
â”‚   â”œâ”€â”€ creators/           # Profile twÃ³rcÃ³w jako osobne pliki JSON ({username}.json)
â”‚   â”œâ”€â”€ categories/         # Dane kategorii
â”‚   â”œâ”€â”€ exports/            # Eksporty CSV
â”‚   â””â”€â”€ checkpoint.json     # Checkpoint dla resume capability
â”œâ”€â”€ tests/                 # Testy jednostkowe
â”œâ”€â”€ scripts/               # Skrypty pomocnicze
â”‚   â”œâ”€â”€ export_data.py     # Export do CSV
â”‚   â””â”€â”€ setup_db.py        # Setup bazy danych
â”œâ”€â”€ .github/workflows/     # GitHub Actions
â”‚   â”œâ”€â”€ scrape.yml         # Scheduled scraping
â”‚   â””â”€â”€ ci.yml             # CI/CD
â””â”€â”€ logs/                  # Logi scrapera
```

SzczegÃ³Å‚owa struktura: [PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)

## ğŸ¯ Kluczowe FunkcjonalnoÅ›ci

### Normalizacja Danych (Opcja B)
Scraper zapisuje zarÃ³wno formaty surowe z HTML jak i znormalizowane:
- **Daty**: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
- **Statystyki**: `{"raw": "19.8K Views", "normalized": 19800}`

Zapewnia to elastycznoÅ›Ä‡ w analizie i moÅ¼liwoÅ›Ä‡ weryfikacji danych ÅºrÃ³dÅ‚owych.

### Checkpoint System
Scraper automatycznie zapisuje postÄ™p scrapowania, umoÅ¼liwiajÄ…c wznowienie po przerwie:
- Automatyczne pomijanie juÅ¼ przetworzonych URL-i
- Åšledzenie nieudanych URL-i do ponownego przetworzenia
- Zapisywanie statystyk w checkpointie

### Zapisywanie Profili KreatorÃ³w
Profile kreatorÃ³w sÄ… zapisywane jako osobne pliki JSON:
- Lokalizacja: `data/creators/{username}.json`
- KaÅ¼dy kreator ma jeden plik, nawet jeÅ›li ma wiele produktÃ³w
- Zawiera peÅ‚ne dane: bio, avatar, stats, social media
- MoÅ¼na eksportowaÄ‡ do CSV uÅ¼ywajÄ…c `export_creators_to_csv()`

**Techniczne szczegÃ³Å‚y parsowania:**
- **Avatar**: WyciÄ…gany z JSON danych Next.js (priorytet), pomijane placeholdery API (`api/og/creator`)
- **Social Media**: WyciÄ…gane z JSON danych Next.js, automatycznie filtrowane linki Framer. ObsÅ‚ugiwane platformy: Twitter/X, LinkedIn, Instagram, GitHub, Dribbble, Behance, YouTube

### ObsÅ‚uga RÃ³Å¼nych TypÃ³w ProduktÃ³w
KaÅ¼dy typ produktu ma unikalne pola i statystyki:
- **Templates**: Pages + Views
- **Plugins**: Version + Users + Changelog
- **Components**: Installs (wyciÄ…gane z JSON danych Next.js lub HTML tekstu)
- **Vectors**: Users + Views + Vectors (count)

## ğŸ“Š PrzykÅ‚adowe Komendy

### Scrapowanie produktÃ³w

```bash
# Scrapowanie wszystkich typÃ³w produktÃ³w
python3 -m src.main

# Scrapowanie z limitem (test)
python3 -m src.main 10

# Scrapowanie tylko okreÅ›lonych typÃ³w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki
```

### Scrapowanie kreatorÃ³w

```bash
# Wszyscy kreatorzy
python3 -m src.main --creators-only

# Z limitem
python3 -m src.main --creators-only 10
python3 -m src.main -c 10  # KrÃ³tka wersja
```

### Scrapowanie kategorii

```bash
# Wszystkie kategorie
python3 -m src.main --categories-only

# Z limitem
python3 -m src.main --categories-only 10
python3 -m src.main -cat 10  # KrÃ³tka wersja
```

### Export danych

```bash
# Export wszystkich produktÃ³w do CSV
python scripts/export_data.py -o data/exports/all_products.csv

# Export tylko templates
python scripts/export_data.py --type template -o data/exports/templates.csv

# Export z limitem
python scripts/export_data.py --limit 100 -o data/exports/sample.csv

# Export kreatorÃ³w do CSV
python -c "from src.storage.file_storage import FileStorage; storage = FileStorage(); storage.export_creators_to_csv()"
```

### Inne

```bash
# Setup PostgreSQL database
python scripts/setup_db.py --db-type postgresql

# WymuÅ› nowe scrapowanie (wyczyÅ›Ä‡ checkpoint)
rm data/checkpoint.json
python3 -m src.main
```

## ğŸ” Uwagi Prawne

âš ï¸ **WaÅ¼ne:**
- Przeczytaj Terms of Service Framer przed scrapowaniem
- Respektuj robots.txt
- Nie przeciÄ…Å¼aj serwerÃ³w (rate limiting)
- Nie scrapuj danych osobowych bez zgody
- RozwaÅ¼ kontakt z Framer - mogÄ… oferowaÄ‡ API

## ğŸ“ NastÄ™pne Kroki

1. **Przetestuj lokalnie** - uruchom z limitem 10-20 produktÃ³w
2. **SprawdÅº dane** - zweryfikuj jakoÅ›Ä‡ zebranych danych
3. **Skonfiguruj GitHub Actions** - dodaj secrets jeÅ›li potrzebne
4. **Rozszerz funkcjonalnoÅ›ci** - dodaj testy, monitoring, itp.

## ğŸ¤ Contributing

Projekt jest w fazie rozwoju. Wszelkie sugestie i PR-y sÄ… mile widziane!

## ğŸ“„ License

[TODO: Dodaj licencjÄ™]

---

*Ostatnia aktualizacja: 2024-12-19*

