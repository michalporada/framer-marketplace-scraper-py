# Scraper Framer Marketplace V2

Zaawansowany scraper do zbierania danych z Framer Marketplace, umoÅ¼liwiajÄ…cy automatyzacjÄ™ pobierania informacji o:

- **Produktach**: Szablony (templates), Komponenty (components), Wektory (vectors), **Wtyczki (plugins)** â­
- **TwÃ³rcach/UÅ¼ytkownikach**: Profile z username (moÅ¼e zawieraÄ‡ znaki specjalne)
- **Kategoriach**: Kategorie produktÃ³w w marketplace

## ğŸ“š Dokumentacja

### GÅ‚Ã³wne Dokumenty

1. **[DEPLOYMENT_PLAN.md](./docs/DEPLOYMENT_PLAN.md)** - Kompletny plan wdroÅ¼enia (Vercel + Railway + Supabase):
   - Krok po kroku instrukcje deploymentu
   - Konfiguracja wszystkich serwisÃ³w
   - Troubleshooting i best practices

2. **[STACK_TECHNICZNY.md](./documentation_sources/STACK_TECHNICZNY.md)** - SzczegÃ³Å‚owy opis stacku technicznego, w tym:
   - Biblioteki Python i narzÄ™dzia
   - Opcje baz danych
   - GitHub Actions i Vercel
   - Rekomendacje deployment

3. **[PROPOZYCJA_ARCHITEKTURY.md](./documentation_sources/PROPOZYCJA_ARCHITEKTURY.md)** - Propozycja struktury projektu:
   - Struktura folderÃ³w
   - Opis komponentÃ³w
   - Flow scrapowania
   - Deployment strategy

4. **[REKOMENDACJE_SCRAPERA_FRAMER.md](./documentation_sources/REKOMENDACJE_SCRAPERA_FRAMER.md)** - SzczegÃ³Å‚owa analiza Framer Marketplace:
   - Analiza techniczna strony
   - Struktura URL-i i selektory CSS
   - Zalecane dane do zbierania
   - Uwagi techniczne

## ğŸš€ Quick Start

### 1. Instalacja

```bash
# Sklonuj repozytorium
git clone <repo-url>
cd scraper-v2

# UtwÃ³rz Å›rodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Dla development

# Zainstaluj pre-commit hooks (OBOWIÄ„ZKOWE - zapobiega bÅ‚Ä™dom CI)
pre-commit install
```

### 2. Konfiguracja

```bash
# Skopiuj przykÅ‚adowy plik .env
cp .env.example .env

# Edytuj .env z odpowiednimi wartoÅ›ciami
# WiÄ™kszoÅ›Ä‡ wartoÅ›ci ma sensowne domyÅ›lne ustawienia
```

GÅ‚Ã³wne zmienne Å›rodowiskowe:
- `FRAMER_BASE_URL` - URL do Framer (domyÅ›lnie: https://www.framer.com)
- `RATE_LIMIT` - Limit requestÃ³w na sekundÄ™ (domyÅ›lnie: 1.0)
- `MAX_RETRIES` - Maksymalna liczba ponownych prÃ³b (domyÅ›lnie: 5, z exponential backoff + jitter, max 5 min)
- `TIMEOUT` - Timeout per request w sekundach (domyÅ›lnie: 25s, zakres 20-30s)
- `GLOBAL_SCRAPING_TIMEOUT` - Globalny timeout na caÅ‚y scraping (domyÅ›lnie: 900s = 15 min)
- `LOG_LEVEL` - Poziom logowania (INFO, DEBUG, WARNING, ERROR)
- `CHECKPOINT_ENABLED` - WÅ‚Ä…cz checkpoint system (domyÅ›lnie: true)
- `MIN_URLS_THRESHOLD` - Minimalny prÃ³g URL-i z sitemapa (domyÅ›lnie: 50)
- `SITEMAP_CACHE_ENABLED` - WÅ‚Ä…cz cache sitemap (domyÅ›lnie: true)
- `SITEMAP_CACHE_MAX_AGE` - Maksymalny wiek cache w sekundach (domyÅ›lnie: 3600s = 1h)
- `SCRAPE_TEMPLATES`, `SCRAPE_COMPONENTS`, `SCRAPE_VECTORS`, `SCRAPE_PLUGINS` - Typy produktÃ³w do scrapowania

**Uwaga o retry sequence**: Scraper automatycznie prÃ³buje pobraÄ‡ Å›wieÅ¼Ä… sitemap 15 razy z opÃ³Åºnieniami (ciÄ…g Fibonacciego w sekundach: 0s, 1s, 1s, 2s, 3s, 5s, 8s, 13s, 21s, 34s, 55s, 89s, 144s, 233s, 377s, Å‚Ä…cznie ~16.4 min) przed uÅ¼yciem cache. To daje CloudFront czas na odbudowÄ™ i zwiÄ™ksza szansÄ™ na Å›wieÅ¼e dane.

### 3. Uruchomienie

```bash
# Podstawowe uruchomienie (scrapuje wszystkie produkty)
python3 -m src.main

# Ograniczenie liczby produktÃ³w (np. 10 dla testÃ³w)
python3 -m src.main 10

# Scrapowanie tylko okreÅ›lonych typÃ³w produktÃ³w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki

# Scrapowanie tylko kreatorÃ³w
python3 -m src.main --creators-only        # Wszyscy kreatorzy
python3 -m src.main --creators-only 10     # Z limitem
python3 -m src.main -c 10                  # KrÃ³tka wersja

# Scrapowanie tylko kategorii
python3 -m src.main --categories-only       # Wszystkie kategorie
python3 -m src.main --categories-only 10   # Z limitem
python3 -m src.main -cat 10                # KrÃ³tka wersja

# Export danych do CSV
python scripts/export_data.py

# Export tylko okreÅ›lonego typu produktu
python scripts/export_data.py --type template

# Setup bazy danych (opcjonalnie)
python scripts/setup_db.py --db-type postgresql
```

### 4. GitHub Actions (Automatyzacja)

Scraper moÅ¼e byÄ‡ uruchamiany automatycznie przez GitHub Actions:

- **Scheduled**: Codziennie o 2:00 UTC (zobacz `.github/workflows/scrape.yml`)
- **Manual**: RÄ™czne uruchomienie przez `workflow_dispatch`

**Zachowanie historii:**
- Najnowsze dane: `data/` (nadpisywane przy kaÅ¼dym scrapie)
- Archiwum z datÄ…: `scraped-data-YYYY-MM-DD/` (zachowuje historiÄ™)
- Artifacts w GitHub: 90 dni przechowywania

**Automatyczne porÃ³wnywanie:**
Po kilku dniach scrapowania moÅ¼esz porÃ³wnywaÄ‡ zmiany w czasie przez API (zobacz sekcjÄ™ API poniÅ¼ej).

## ğŸ› ï¸ Stack Techniczny

### Backend
- **Python 3.11+** - jÄ™zyk gÅ‚Ã³wny
- **httpx** - async HTTP client
- **BeautifulSoup4** - parsowanie HTML
- **pydantic v2** - walidacja danych z normalizacjÄ… (Opcja B)
- **pandas** - manipulacja danych i eksport do CSV
- **SQLAlchemy** - ORM (opcjonalnie, dla PostgreSQL)

### NarzÄ™dzia
- **structlog** - strukturalne logowanie
- **tenacity** - retry logic z exponential backoff + jitter (5 retry, max 5 min)
- **fake-useragent** - rotacja User-Agent headers
- **tqdm** - progress bars
- **cachetools** - API caching (TTLCache)

### Deployment & Automation
- **GitHub Actions** - automatyczne scrapowanie (scheduled)
- **Railway** - FastAPI backend (https://framer-marketplace-scraper-py-production.up.railway.app)
- **Vercel** - Next.js frontend (https://framer-marketplace-scraper-py.vercel.app)
- **Supabase** - PostgreSQL database (skonfigurowana)

### Storage
- **JSON/CSV** - podstawowe (zalecane na start)
- **PostgreSQL** - dla wiÄ™kszych projektÃ³w
  - Tabela `products` - najnowsze wersje produktÃ³w
  - Tabela `product_history` - peÅ‚na historia zmian produktÃ³w (kaÅ¼dy scrap tworzy nowy wpis)
  - Tabela `creators` - dane twÃ³rcÃ³w
- **GitHub Artifacts** - backup danych

## ğŸ“‹ FunkcjonalnoÅ›ci

### âœ… Zaimplementowane

- [x] Scrapowanie produktÃ³w z sitemap.xml (templates/components/vectors/**plugins**)
- [x] Scrapowanie danych twÃ³rcÃ³w (profile z `@username`)
- [x] Zapisywanie profilÃ³w twÃ³rcÃ³w jako osobne pliki JSON (`data/creators/{username}.json`)
- [x] Scrapowanie kategorii (opcjonalnie)
- [x] Rate limiting i error handling
- [x] Zapis do JSON/CSV (organizacja wedÅ‚ug typu produktu)
- [x] Eksport kreatorÃ³w do CSV (`export_creators_to_csv()`)
- [x] Automatyzacja przez GitHub Actions
- [x] Resume capability (wznowienie po przerwie) - checkpoint system
- [x] Walidacja danych (Pydantic)
- [x] Monitoring i logowanie (structlog)
- [x] Normalizacja danych (Opcja B - raw + normalized)
- [x] ObsÅ‚uga rÃ³Å¼nych typÃ³w produktÃ³w (rÃ³Å¼ne statystyki i pola)
- [x] Historia produktÃ³w w bazie danych (`product_history` table)
- [x] Automatyczne zapisywanie historii przy kaÅ¼dym scrapie
- [x] Optymalizacja batch operations (transakcje, chunking)
- [x] API caching (cachetools) dla szybkich odpowiedzi
- [x] Prepared statements dla bezpiecznych zapytaÅ„ SQL

### âœ… API Endpoints (FastAPI)

API jest dostÄ™pne i gotowe do uÅ¼ycia:

**PorÃ³wnywanie produktÃ³w w czasie:**
```bash
GET /api/products/{product_id}/changes
```
PorÃ³wnuje dane produktu miÄ™dzy rÃ³Å¼nymi scrapami, wykrywa zmiany w statystykach, cenie i metadanych.

**PorÃ³wnywanie kategorii:**
```bash
GET /api/products/categories/comparison
GET /api/products/categories/comparison?product_type=template
GET /api/products/categories/comparison?category=Agency
```
PorÃ³wnuje Å‚Ä…cznÄ… liczbÄ™ wyÅ›wietleÅ„ kategorii miÄ™dzy scrapami z procentowym wzrostem/spadkiem.

**Monitoring i metryki:**
```bash
GET /api/metrics/summary
```
Zwraca aktualne metryki scrapera (liczba scrapowanych produktÃ³w, czas, success rate).

```bash
GET /api/metrics/history?limit=50&offset=0
```
Zwraca historyczne metryki z pliku `metrics.log` z paginacjÄ….

```bash
GET /api/metrics/stats
```
Zwraca poÅ‚Ä…czone statystyki: metryki scrapera, cache stats i statystyki bazy danych.

**ZarzÄ…dzanie cache:**
```bash
GET /cache/stats
```
Zwraca statystyki cache (rozmiar, TTL, hit rate).

```bash
POST /cache/invalidate?type=product|creator|all
```
CzyÅ›ci cache (product, creator lub wszystkie).

**Inne endpointy:**
- `GET /api/products` - lista produktÃ³w (z cache, prepared statements)
- `GET /api/products/{id}` - pojedynczy produkt (z cache)
- `GET /api/creators` - lista twÃ³rcÃ³w (z cache, prepared statements)
- `GET /api/creators/{username}` - pojedynczy twÃ³rca (z cache)

### ğŸ”® Opcjonalne (Faza 2+)

- [ ] Dashboard (Next.js)
- [ ] Baza danych (PostgreSQL) - setup script gotowy
- [ ] Error tracking (Sentry)
- [ ] Notyfikacje (Slack/Email)

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/          # Scrapery (sitemap, product, creator, category)
â”‚   â”œâ”€â”€ parsers/           # Parsery HTML (product, creator, category)
â”‚   â”œâ”€â”€ models/            # Modele Pydantic (Product, Creator, Category)
â”‚   â”œâ”€â”€ storage/           # Zapis danych (file_storage, database)
â”‚   â”œâ”€â”€ utils/             # NarzÄ™dzia (logger, rate_limiter, retry, normalizers, checkpoint)
â”‚   â”œâ”€â”€ config/            # Konfiguracja (settings)
â”‚   â””â”€â”€ main.py            # Entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/          # Zapisane produkty (templates/, components/, vectors/, plugins/)
â”‚   â”œâ”€â”€ creators/           # Profile twÃ³rcÃ³w jako osobne pliki JSON ({username}.json)
â”‚   â”œâ”€â”€ categories/         # Dane kategorii
â”‚   â”œâ”€â”€ exports/            # Eksporty CSV
â”‚   â””â”€â”€ checkpoint.json     # Checkpoint dla resume capability
â”œâ”€â”€ tests/                 # Testy jednostkowe
â”œâ”€â”€ scripts/               # Skrypty pomocnicze
â”‚   â”œâ”€â”€ export_data.py     # Export do CSV
â”‚   â””â”€â”€ setup_db.py        # Setup bazy danych
â”œâ”€â”€ .github/workflows/     # GitHub Actions
â”‚   â”œâ”€â”€ scrape.yml         # Scheduled scraping
â”‚   â””â”€â”€ ci.yml             # CI/CD
â””â”€â”€ logs/                  # Logi scrapera
```

SzczegÃ³Å‚owa struktura: [PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)

## ğŸ¯ Kluczowe FunkcjonalnoÅ›ci

### Normalizacja Danych (Opcja B)
Scraper zapisuje zarÃ³wno formaty surowe z HTML jak i znormalizowane:
- **Daty**: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
- **Statystyki**: `{"raw": "19.8K Views", "normalized": 19800}`

Zapewnia to elastycznoÅ›Ä‡ w analizie i moÅ¼liwoÅ›Ä‡ weryfikacji danych ÅºrÃ³dÅ‚owych.

### Checkpoint System
Scraper automatycznie zapisuje postÄ™p scrapowania, umoÅ¼liwiajÄ…c wznowienie po przerwie:
- Automatyczne pomijanie juÅ¼ przetworzonych URL-i
- Åšledzenie nieudanych URL-i do ponownego przetworzenia
- Zapisywanie statystyk w checkpointie

### Zapisywanie Profili KreatorÃ³w
Profile kreatorÃ³w sÄ… zapisywane jako osobne pliki JSON:
- Lokalizacja: `data/creators/{username}.json`
- KaÅ¼dy kreator ma jeden plik, nawet jeÅ›li ma wiele produktÃ³w
- Zawiera peÅ‚ne dane: bio, avatar, stats, social media
- MoÅ¼na eksportowaÄ‡ do CSV uÅ¼ywajÄ…c `export_creators_to_csv()`

**Techniczne szczegÃ³Å‚y parsowania:**
- **Avatar**: WyciÄ…gany z JSON danych Next.js (priorytet), pomijane placeholdery API (`api/og/creator`)
- **Social Media**: WyciÄ…gane z JSON danych Next.js, automatycznie filtrowane linki Framer. ObsÅ‚ugiwane platformy: Twitter/X, LinkedIn, Instagram, GitHub, Dribbble, Behance, YouTube

### ObsÅ‚uga RÃ³Å¼nych TypÃ³w ProduktÃ³w
KaÅ¼dy typ produktu ma unikalne pola i statystyki:
- **Templates**: Pages + Views
- **Plugins**: Version + Users + Changelog
- **Components**: Installs (wyciÄ…gane z JSON danych Next.js lub HTML tekstu)
- **Vectors**: Users + Views + Vectors (count)

### Historia ProduktÃ³w (Product History)
Scraper automatycznie zapisuje peÅ‚nÄ… historiÄ™ zmian produktÃ³w do tabeli `product_history` w bazie danych:
- **KaÅ¼dy scrap tworzy nowy wpis** - nigdy nie nadpisuje istniejÄ…cych danych
- **Timestamp `scraped_at`** - pozwala Å›ledziÄ‡ zmiany w czasie
- **PeÅ‚ne dane produktu** - wszystkie pola (statystyki, cena, metadane) sÄ… zapisywane
- **Analiza trendÃ³w** - moÅ¼esz porÃ³wnywaÄ‡ dane z rÃ³Å¼nych dat przez API (`/api/products/{id}/changes`)
- **Automatyczne zapisywanie** - dziaÅ‚a przy kaÅ¼dym scrapowaniu (single i batch)

### API Caching
API uÅ¼ywa cache (cachetools) dla szybkich odpowiedzi:
- **TTL: 5 minut** - domyÅ›lny czas Å¼ycia cache
- **Osobne cache** - dla produktÃ³w i twÃ³rcÃ³w
- **Automatyczne invalidation** - moÅ¼esz wyczyÅ›ciÄ‡ cache przez endpoint
- **Statystyki** - dostÄ™pne przez `/cache/stats`

### Optymalizacja Batch Operations
Zapisywanie wielu produktÃ³w/twÃ³rcÃ³w jest zoptymalizowane:
- **Transakcje SQL** - wszystkie operacje w jednej transakcji
- **Chunking** - duÅ¼e batchy (>1000) sÄ… dzielone na mniejsze czÄ™Å›ci
- **Prepared statements** - bezpieczne zapytania SQL (ochrona przed SQL injection)
- **Automatyczne zapisywanie historii** - kaÅ¼dy produkt w batch jest zapisywany do `product_history`

## ğŸ“Š PrzykÅ‚adowe Komendy

### Scrapowanie produktÃ³w

```bash
# Scrapowanie wszystkich typÃ³w produktÃ³w
python3 -m src.main

# Scrapowanie z limitem (test)
python3 -m src.main 10

# Scrapowanie tylko okreÅ›lonych typÃ³w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki
```

### Scrapowanie kreatorÃ³w

```bash
# Wszyscy kreatorzy
python3 -m src.main --creators-only

# Z limitem
python3 -m src.main --creators-only 10
python3 -m src.main -c 10  # KrÃ³tka wersja
```

### Scrapowanie kategorii

```bash
# Wszystkie kategorie
python3 -m src.main --categories-only

# Z limitem
python3 -m src.main --categories-only 10
python3 -m src.main -cat 10  # KrÃ³tka wersja
```

### Export danych

```bash
# Export wszystkich produktÃ³w do CSV
python scripts/export_data.py -o data/exports/all_products.csv

# Export tylko templates
python scripts/export_data.py --type template -o data/exports/templates.csv

# Export z limitem
python scripts/export_data.py --limit 100 -o data/exports/sample.csv

# Export kreatorÃ³w do CSV
python -c "from src.storage.file_storage import FileStorage; storage = FileStorage(); storage.export_creators_to_csv()"
```

### Inne

```bash
# Setup PostgreSQL database
python scripts/setup_db.py --db-type postgresql

# WymuÅ› nowe scrapowanie (wyczyÅ›Ä‡ checkpoint)
rm data/checkpoint.json
python3 -m src.main
```

## ğŸ” Uwagi Prawne

âš ï¸ **WaÅ¼ne:**
- Przeczytaj Terms of Service Framer przed scrapowaniem
- Respektuj robots.txt
- Nie przeciÄ…Å¼aj serwerÃ³w (rate limiting)
- Nie scrapuj danych osobowych bez zgody
- RozwaÅ¼ kontakt z Framer - mogÄ… oferowaÄ‡ API

## ğŸ“ NastÄ™pne Kroki

1. **Przetestuj lokalnie** - uruchom z limitem 10-20 produktÃ³w
2. **SprawdÅº dane** - zweryfikuj jakoÅ›Ä‡ zebranych danych
3. **Skonfiguruj GitHub Actions** - dodaj secrets jeÅ›li potrzebne
4. **Rozszerz funkcjonalnoÅ›ci** - dodaj testy, monitoring, itp.

## ğŸ¤ Contributing

Projekt jest w fazie rozwoju. Wszelkie sugestie i PR-y sÄ… mile widziane!

## ğŸ“„ License

[TODO: Dodaj licencjÄ™]

---

*Ostatnia aktualizacja: 2024-12-19*

---

## ğŸ“Š Historia ProduktÃ³w i Analiza TrendÃ³w

### Jak dziaÅ‚a Product History

Scraper automatycznie zapisuje kaÅ¼dÄ… wersjÄ™ produktu do tabeli `product_history` w bazie danych PostgreSQL. DziÄ™ki temu moÅ¼esz:

1. **ÅšledziÄ‡ zmiany w czasie** - kaÅ¼dy scrap tworzy nowy wpis z timestampem `scraped_at`
2. **AnalizowaÄ‡ trendy** - porÃ³wnywaÄ‡ statystyki (views, pages, users, installs) miÄ™dzy scrapami
3. **WykrywaÄ‡ wzrosty/spadki** - zobacz jak zmienia siÄ™ popularnoÅ›Ä‡ produktÃ³w i kategorii

### PrzykÅ‚adowe uÅ¼ycie

```bash
# SprawdÅº zmiany produktu w czasie
GET /api/products/{product_id}/changes

# PorÃ³wnaj trendy kategorii
GET /api/products/categories/comparison?category=Agency

# SprawdÅº metryki scrapera
GET /api/metrics/stats
```

### Synchronizacja istniejÄ…cych danych

JeÅ›li masz juÅ¼ produkty w tabeli `products`, moÅ¼esz zsynchronizowaÄ‡ je do `product_history`:

```bash
python scripts/sync_existing_to_history.py
```

Ten skrypt:
- Åaduje wszystkie produkty z tabeli `products`
- Wstawia je do `product_history` z aktualnym timestampem
- Pomija duplikaty (na podstawie `product_id` i `scraped_at`)

