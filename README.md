# Scraper Framer Marketplace V2

Zaawansowany scraper do zbierania danych z Framer Marketplace, umo≈ºliwiajƒÖcy automatyzacjƒô pobierania informacji o:

- **Produktach**: Szablony (templates), Komponenty (components), Wektory (vectors), **Wtyczki (plugins)** ‚≠ê
- **Tw√≥rcach/U≈ºytkownikach**: Profile z username (mo≈ºe zawieraƒá znaki specjalne)
- **Kategoriach**: Kategorie produkt√≥w w marketplace

## üìö Dokumentacja

### G≈Ç√≥wne Dokumenty

1. **[DEPLOYMENT_PLAN.md](./docs/DEPLOYMENT_PLAN.md)** - Kompletny plan wdro≈ºenia (Vercel + Railway + Supabase):
   - Krok po kroku instrukcje deploymentu
   - Konfiguracja wszystkich serwis√≥w
   - Troubleshooting i best practices

2. **[API_ENDPOINTS_LIST.md](./docs/API_ENDPOINTS_LIST.md)** - Pe≈Çna lista wszystkich endpoint√≥w API:
   - 22 endpointy z opisami i przyk≈Çadami
   - Query parameters i response models
   - Error codes i cache status

3. **[API_PRODUCTS_EXAMPLES.md](./docs/API_PRODUCTS_EXAMPLES.md)** - Przyk≈Çady u≈ºycia endpoint√≥w Products:
   - R√≥≈ºne typy produkt√≥w (templates, components, vectors, plugins)
   - Przyk≈Çady curl i Python
   - Analiza zmian views w 24h

4. **[API_CREATORS_ANALYSIS_EXAMPLES.md](./docs/API_CREATORS_ANALYSIS_EXAMPLES.md)** - Przyk≈Çady analizy danych kreator√≥w:
   - Analiza wzrostu views produkt√≥w kreatora
   - Przyk≈Çady dla r√≥≈ºnych okres√≥w i typ√≥w produkt√≥w

5. **[API_CATEGORIES_VIEWS_EXAMPLES.md](./docs/API_CATEGORIES_VIEWS_EXAMPLES.md)** - Przyk≈Çady sprawdzania views kategorii:
   - Statystyki kategorii
   - Por√≥wnywanie kategorii
   - Top produkty w kategorii

6. **[STACK_TECHNICZNY.md](./documentation_sources/STACK_TECHNICZNY.md)** - Szczeg√≥≈Çowy opis stacku technicznego, w tym:
   - Biblioteki Python i narzƒôdzia
   - Opcje baz danych
   - GitHub Actions i Vercel
   - Rekomendacje deployment

7. **[PROPOZYCJA_ARCHITEKTURY.md](./documentation_sources/PROPOZYCJA_ARCHITEKTURY.md)** - Propozycja struktury projektu:
   - Struktura folder√≥w
   - Opis komponent√≥w
   - Flow scrapowania
   - Deployment strategy

8. **[REKOMENDACJE_SCRAPERA_FRAMER.md](./documentation_sources/REKOMENDACJE_SCRAPERA_FRAMER.md)** - Szczeg√≥≈Çowa analiza Framer Marketplace:
   - Analiza techniczna strony
   - Struktura URL-i i selektory CSS
   - Zalecane dane do zbierania
   - Uwagi techniczne

## üöÄ Quick Start

### 1. Instalacja

```bash
# Sklonuj repozytorium
git clone <repo-url>
cd scraper-v2

# Utw√≥rz ≈õrodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows

# Zainstaluj zale≈ºno≈õci
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Dla development

# Zainstaluj pre-commit hooks (OBOWIƒÑZKOWE - zapobiega b≈Çƒôdom CI)
pre-commit install
```

### 2. Konfiguracja

```bash
# Skopiuj przyk≈Çadowy plik .env
cp .env.example .env

# Edytuj .env z odpowiednimi warto≈õciami
# Wiƒôkszo≈õƒá warto≈õci ma sensowne domy≈õlne ustawienia
```

G≈Ç√≥wne zmienne ≈õrodowiskowe:
- `FRAMER_BASE_URL` - URL do Framer (domy≈õlnie: https://www.framer.com)
- `RATE_LIMIT` - Limit request√≥w na sekundƒô (domy≈õlnie: 1.0)
- `MAX_RETRIES` - Maksymalna liczba ponownych pr√≥b (domy≈õlnie: 5, z exponential backoff + jitter, max 5 min)
- `TIMEOUT` - Timeout per request w sekundach (domy≈õlnie: 25s, zakres 20-30s)
- `GLOBAL_SCRAPING_TIMEOUT` - Globalny timeout na ca≈Çy scraping (domy≈õlnie: 900s = 15 min)
- `LOG_LEVEL` - Poziom logowania (INFO, DEBUG, WARNING, ERROR)
- `CHECKPOINT_ENABLED` - W≈ÇƒÖcz checkpoint system (domy≈õlnie: true)
- `MIN_URLS_THRESHOLD` - Minimalny pr√≥g URL-i z sitemapa (domy≈õlnie: 50)
- `SITEMAP_CACHE_ENABLED` - W≈ÇƒÖcz cache sitemap (domy≈õlnie: true)
- `SITEMAP_CACHE_MAX_AGE` - Maksymalny wiek cache w sekundach (domy≈õlnie: 3600s = 1h)
- `SCRAPE_TEMPLATES`, `SCRAPE_COMPONENTS`, `SCRAPE_VECTORS`, `SCRAPE_PLUGINS` - Typy produkt√≥w do scrapowania

**Uwaga o retry sequence**: Scraper automatycznie pr√≥buje pobraƒá ≈õwie≈ºƒÖ sitemap 15 razy z op√≥≈∫nieniami (ciƒÖg Fibonacciego w sekundach: 0s, 1s, 1s, 2s, 3s, 5s, 8s, 13s, 21s, 34s, 55s, 89s, 144s, 233s, 377s, ≈ÇƒÖcznie ~16.4 min) przed u≈ºyciem cache. To daje CloudFront czas na odbudowƒô i zwiƒôksza szansƒô na ≈õwie≈ºe dane.

### 3. Uruchomienie

```bash
# Podstawowe uruchomienie (scrapuje wszystkie produkty)
python3 -m src.main

# Ograniczenie liczby produkt√≥w (np. 10 dla test√≥w)
python3 -m src.main 10

# Scrapowanie tylko okre≈õlonych typ√≥w produkt√≥w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki

# Scrapowanie tylko kreator√≥w
python3 -m src.main --creators-only        # Wszyscy kreatorzy
python3 -m src.main --creators-only 10     # Z limitem
python3 -m src.main -c 10                  # Kr√≥tka wersja

# Scrapowanie tylko kategorii
python3 -m src.main --categories-only       # Wszystkie kategorie
python3 -m src.main --categories-only 10   # Z limitem
python3 -m src.main -cat 10                # Kr√≥tka wersja

# Export danych do CSV
python scripts/export_data.py

# Export tylko okre≈õlonego typu produktu
python scripts/export_data.py --type template

# Setup bazy danych (opcjonalnie)
python scripts/setup_db.py --db-type postgresql
```

### 4. GitHub Actions (Automatyzacja)

Scraper mo≈ºe byƒá uruchamiany automatycznie przez GitHub Actions:

- **Scheduled**: Codziennie o 2:00 UTC (zobacz `.github/workflows/scrape.yml`)
- **Manual**: Rƒôczne uruchomienie przez `workflow_dispatch`

**Zachowanie historii:**
- Najnowsze dane: `data/` (nadpisywane przy ka≈ºdym scrapie)
- Archiwum z datƒÖ: `scraped-data-YYYY-MM-DD/` (zachowuje historiƒô)
- Artifacts w GitHub: 90 dni przechowywania

**Automatyczne por√≥wnywanie:**
Po kilku dniach scrapowania mo≈ºesz por√≥wnywaƒá zmiany w czasie przez API (zobacz sekcjƒô API poni≈ºej).

## üõ†Ô∏è Stack Techniczny

### Backend
- **Python 3.11+** - jƒôzyk g≈Ç√≥wny
- **httpx** - async HTTP client
- **BeautifulSoup4** - parsowanie HTML
- **pydantic v2** - walidacja danych z normalizacjƒÖ (Opcja B)
- **pandas** - manipulacja danych i eksport do CSV
- **SQLAlchemy** - ORM (opcjonalnie, dla PostgreSQL)

### Narzƒôdzia
- **structlog** - strukturalne logowanie
- **tenacity** - retry logic z exponential backoff + jitter (5 retry, max 5 min)
- **fake-useragent** - rotacja User-Agent headers
- **tqdm** - progress bars
- **cachetools** - API caching (TTLCache)

### Deployment & Automation
- **GitHub Actions** - automatyczne scrapowanie (scheduled)
- **Railway** - FastAPI backend (https://framer-marketplace-scraper-py-production.up.railway.app)
- **Vercel** - Next.js frontend (https://framer-marketplace-scraper-py.vercel.app)
- **Supabase** - PostgreSQL database (skonfigurowana)

### Storage
- **JSON/CSV** - podstawowe (zalecane na start)
- **PostgreSQL** - dla wiƒôkszych projekt√≥w
  - Tabela `products` - najnowsze wersje produkt√≥w
  - Tabela `product_history` - pe≈Çna historia zmian produkt√≥w (ka≈ºdy scrap tworzy nowy wpis)
  - Tabela `creators` - dane tw√≥rc√≥w
- **GitHub Artifacts** - backup danych

## üìã Funkcjonalno≈õci

### ‚úÖ Zaimplementowane

- [x] Scrapowanie produkt√≥w z sitemap.xml (templates/components/vectors/**plugins**)
- [x] Scrapowanie danych tw√≥rc√≥w (profile z `@username`)
- [x] Zapisywanie profil√≥w tw√≥rc√≥w jako osobne pliki JSON (`data/creators/{username}.json`)
- [x] Scrapowanie kategorii (opcjonalnie)
- [x] Rate limiting i error handling
- [x] Zapis do JSON/CSV (organizacja wed≈Çug typu produktu)
- [x] Eksport kreator√≥w do CSV (`export_creators_to_csv()`)
- [x] Automatyzacja przez GitHub Actions
- [x] Resume capability (wznowienie po przerwie) - checkpoint system
- [x] Walidacja danych (Pydantic)
- [x] Monitoring i logowanie (structlog)
- [x] Normalizacja danych (Opcja B - raw + normalized)
- [x] Obs≈Çuga r√≥≈ºnych typ√≥w produkt√≥w (r√≥≈ºne statystyki i pola)
- [x] Historia produkt√≥w w bazie danych (`product_history` table)
- [x] Automatyczne zapisywanie historii przy ka≈ºdym scrapie
- [x] Optymalizacja batch operations (transakcje, chunking)
- [x] API caching (cachetools) dla szybkich odpowiedzi
- [x] Prepared statements dla bezpiecznych zapyta≈Ñ SQL

### ‚úÖ API Endpoints (FastAPI)

API jest dostƒôpne i gotowe do u≈ºycia. **Pe≈Çna dokumentacja:** [docs/API_ENDPOINTS_LIST.md](./docs/API_ENDPOINTS_LIST.md)

**G≈Ç√≥wne funkcjonalno≈õci:**

**Por√≥wnywanie produkt√≥w w czasie:**
```bash
GET /api/products/{product_id}/changes
```
Por√≥wnuje dane produktu miƒôdzy r√≥≈ºnymi scrapami, wykrywa zmiany w statystykach, cenie i metadanych.

**Analiza zmian views w 24h:**
```bash
GET /api/products/views-change-24h?product_type=template
```
Oblicza ≈ÇƒÖcznƒÖ zmianƒô views dla wszystkich produkt√≥w danego typu w ostatnich 24 godzinach.

**Views kategorii:**
```bash
GET /api/products/categories/{category_name}/views?product_type=template
```
Zwraca aktualnƒÖ liczbƒô views i statystyki dla danej kategorii.

**Analiza wzrostu produkt√≥w kreatora:**
```bash
GET /api/creators/{username}/products-growth?product_type=template&period_hours=24
```
Analizuje wzrost views dla wszystkich produkt√≥w danego kreatora w okre≈õlonym okresie.

**Por√≥wnywanie kategorii:**
```bash
GET /api/products/categories/comparison
GET /api/products/categories/comparison?product_type=template
GET /api/products/categories/comparison?category=Agency
```
Por√≥wnuje ≈ÇƒÖcznƒÖ liczbƒô wy≈õwietle≈Ñ kategorii miƒôdzy scrapami z procentowym wzrostem/spadkiem.

**Monitoring i metryki:**
```bash
GET /api/metrics/summary
```
Zwraca aktualne metryki scrapera (liczba scrapowanych produkt√≥w, czas, success rate).

```bash
GET /api/metrics/history?limit=50&offset=0
```
Zwraca historyczne metryki z pliku `metrics.log` z paginacjƒÖ.

```bash
GET /api/metrics/stats
```
Zwraca po≈ÇƒÖczone statystyki: metryki scrapera, cache stats i statystyki bazy danych.

**ZarzƒÖdzanie cache:**
```bash
GET /cache/stats
```
Zwraca statystyki cache (rozmiar, TTL, hit rate).

```bash
POST /cache/invalidate?type=product|creator|all
```
Czy≈õci cache (product, creator lub wszystkie).

**Inne endpointy:**
- `GET /api/products` - lista produkt√≥w (z cache, prepared statements)
- `GET /api/products/{id}` - pojedynczy produkt (z cache)
- `GET /api/creators` - lista tw√≥rc√≥w (z cache, prepared statements)
- `GET /api/creators/{username}` - pojedynczy tw√≥rca (z cache)

### üîÆ Opcjonalne (Faza 2+)

- [ ] Dashboard (Next.js)
- [ ] Baza danych (PostgreSQL) - setup script gotowy
- [ ] Error tracking (Sentry)
- [ ] Notyfikacje (Slack/Email)

## üìÅ Struktura Projektu

```
scraper-v2/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/          # Scrapery (sitemap, product, creator, category)
‚îÇ   ‚îú‚îÄ‚îÄ parsers/           # Parsery HTML (product, creator, category)
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Modele Pydantic (Product, Creator, Category)
‚îÇ   ‚îú‚îÄ‚îÄ storage/           # Zapis danych (file_storage, database)
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Narzƒôdzia (logger, rate_limiter, retry, normalizers, checkpoint)
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Konfiguracja (settings)
‚îÇ   ‚îî‚îÄ‚îÄ main.py            # Entry point
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ products/          # Zapisane produkty (templates/, components/, vectors/, plugins/)
‚îÇ   ‚îú‚îÄ‚îÄ creators/           # Profile tw√≥rc√≥w jako osobne pliki JSON ({username}.json)
‚îÇ   ‚îú‚îÄ‚îÄ categories/         # Dane kategorii
‚îÇ   ‚îú‚îÄ‚îÄ exports/            # Eksporty CSV
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint.json     # Checkpoint dla resume capability
‚îú‚îÄ‚îÄ tests/                 # Testy jednostkowe
‚îú‚îÄ‚îÄ scripts/               # Skrypty pomocnicze
‚îÇ   ‚îú‚îÄ‚îÄ export_data.py     # Export do CSV
‚îÇ   ‚îî‚îÄ‚îÄ setup_db.py        # Setup bazy danych
‚îú‚îÄ‚îÄ .github/workflows/     # GitHub Actions
‚îÇ   ‚îú‚îÄ‚îÄ scrape.yml         # Scheduled scraping
‚îÇ   ‚îî‚îÄ‚îÄ ci.yml             # CI/CD
‚îî‚îÄ‚îÄ logs/                  # Logi scrapera
```

Szczeg√≥≈Çowa struktura: [PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)

## üéØ Kluczowe Funkcjonalno≈õci

### Normalizacja Danych (Opcja B)
Scraper zapisuje zar√≥wno formaty surowe z HTML jak i znormalizowane:
- **Daty**: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
- **Statystyki**: `{"raw": "19.8K Views", "normalized": 19800}`

Zapewnia to elastyczno≈õƒá w analizie i mo≈ºliwo≈õƒá weryfikacji danych ≈∫r√≥d≈Çowych.

### Checkpoint System
Scraper automatycznie zapisuje postƒôp scrapowania, umo≈ºliwiajƒÖc wznowienie po przerwie:
- Automatyczne pomijanie ju≈º przetworzonych URL-i
- ≈öledzenie nieudanych URL-i do ponownego przetworzenia
- Zapisywanie statystyk w checkpointie

### Zapisywanie Profili Kreator√≥w
Profile kreator√≥w sƒÖ zapisywane jako osobne pliki JSON:
- Lokalizacja: `data/creators/{username}.json`
- Ka≈ºdy kreator ma jeden plik, nawet je≈õli ma wiele produkt√≥w
- Zawiera pe≈Çne dane: bio, avatar, stats, social media
- Mo≈ºna eksportowaƒá do CSV u≈ºywajƒÖc `export_creators_to_csv()`

**Techniczne szczeg√≥≈Çy parsowania:**
- **Avatar**: WyciƒÖgany z JSON danych Next.js (priorytet), pomijane placeholdery API (`api/og/creator`)
- **Social Media**: WyciƒÖgane z JSON danych Next.js, automatycznie filtrowane linki Framer. Obs≈Çugiwane platformy: Twitter/X, LinkedIn, Instagram, GitHub, Dribbble, Behance, YouTube

### Obs≈Çuga R√≥≈ºnych Typ√≥w Produkt√≥w
Ka≈ºdy typ produktu ma unikalne pola i statystyki:
- **Templates**: Pages + Views
- **Plugins**: Version + Users + Changelog
- **Components**: Installs (wyciƒÖgane z JSON danych Next.js lub HTML tekstu)
- **Vectors**: Users + Views + Vectors (count)

### Historia Produkt√≥w (Product History)
Scraper automatycznie zapisuje pe≈ÇnƒÖ historiƒô zmian produkt√≥w do tabeli `product_history` w bazie danych:
- **Ka≈ºdy scrap tworzy nowy wpis** - nigdy nie nadpisuje istniejƒÖcych danych
- **Timestamp `scraped_at`** - pozwala ≈õledziƒá zmiany w czasie
- **Pe≈Çne dane produktu** - wszystkie pola (statystyki, cena, metadane) sƒÖ zapisywane
- **Analiza trend√≥w** - mo≈ºesz por√≥wnywaƒá dane z r√≥≈ºnych dat przez API (`/api/products/{id}/changes`)
- **Automatyczne zapisywanie** - dzia≈Ça przy ka≈ºdym scrapowaniu (single i batch)

### API Caching
API u≈ºywa cache (cachetools) dla szybkich odpowiedzi:
- **TTL: 5 minut** - domy≈õlny czas ≈ºycia cache
- **Osobne cache** - dla produkt√≥w i tw√≥rc√≥w
- **Automatyczne invalidation** - mo≈ºesz wyczy≈õciƒá cache przez endpoint
- **Statystyki** - dostƒôpne przez `/cache/stats`

### Optymalizacja Batch Operations
Zapisywanie wielu produkt√≥w/tw√≥rc√≥w jest zoptymalizowane:
- **Transakcje SQL** - wszystkie operacje w jednej transakcji
- **Chunking** - du≈ºe batchy (>1000) sƒÖ dzielone na mniejsze czƒô≈õci
- **Prepared statements** - bezpieczne zapytania SQL (ochrona przed SQL injection)
- **Automatyczne zapisywanie historii** - ka≈ºdy produkt w batch jest zapisywany do `product_history`

## üìä Przyk≈Çadowe Komendy

### Scrapowanie produkt√≥w

```bash
# Scrapowanie wszystkich typ√≥w produkt√≥w
python3 -m src.main

# Scrapowanie z limitem (test)
python3 -m src.main 10

# Scrapowanie tylko okre≈õlonych typ√≥w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10   # Tylko komponenty
python3 -m src.main --vectors-only 10      # Tylko wektory
python3 -m src.main --plugins-only 10      # Tylko wtyczki
```

### Scrapowanie kreator√≥w

```bash
# Wszyscy kreatorzy
python3 -m src.main --creators-only

# Z limitem
python3 -m src.main --creators-only 10
python3 -m src.main -c 10  # Kr√≥tka wersja
```

### Scrapowanie kategorii

```bash
# Wszystkie kategorie
python3 -m src.main --categories-only

# Z limitem
python3 -m src.main --categories-only 10
python3 -m src.main -cat 10  # Kr√≥tka wersja
```

### Export danych

```bash
# Export wszystkich produkt√≥w do CSV
python scripts/export_data.py -o data/exports/all_products.csv

# Export tylko templates
python scripts/export_data.py --type template -o data/exports/templates.csv

# Export z limitem
python scripts/export_data.py --limit 100 -o data/exports/sample.csv

# Export kreator√≥w do CSV
python -c "from src.storage.file_storage import FileStorage; storage = FileStorage(); storage.export_creators_to_csv()"
```

### Inne

```bash
# Setup PostgreSQL database
python scripts/setup_db.py --db-type postgresql

# Wymu≈õ nowe scrapowanie (wyczy≈õƒá checkpoint)
rm data/checkpoint.json
python3 -m src.main
```

## üîê Uwagi Prawne

‚ö†Ô∏è **Wa≈ºne:**
- Przeczytaj Terms of Service Framer przed scrapowaniem
- Respektuj robots.txt
- Nie przeciƒÖ≈ºaj serwer√≥w (rate limiting)
- Nie scrapuj danych osobowych bez zgody
- Rozwa≈º kontakt z Framer - mogƒÖ oferowaƒá API

## üìù Nastƒôpne Kroki

1. **Przetestuj lokalnie** - uruchom z limitem 10-20 produkt√≥w
2. **Sprawd≈∫ dane** - zweryfikuj jako≈õƒá zebranych danych
3. **Skonfiguruj GitHub Actions** - dodaj secrets je≈õli potrzebne
4. **Rozszerz funkcjonalno≈õci** - dodaj testy, monitoring, itp.

## ü§ù Contributing

Projekt jest w fazie rozwoju. Wszelkie sugestie i PR-y sƒÖ mile widziane!

## üìÑ License

[TODO: Dodaj licencjƒô]

---

*Ostatnia aktualizacja: 2024-12-19*

---

## üìä Historia Produkt√≥w i Analiza Trend√≥w

### Jak dzia≈Ça Product History

Scraper automatycznie zapisuje ka≈ºdƒÖ wersjƒô produktu do tabeli `product_history` w bazie danych PostgreSQL. Dziƒôki temu mo≈ºesz:

1. **≈öledziƒá zmiany w czasie** - ka≈ºdy scrap tworzy nowy wpis z timestampem `scraped_at`
2. **Analizowaƒá trendy** - por√≥wnywaƒá statystyki (views, pages, users, installs) miƒôdzy scrapami
3. **Wykrywaƒá wzrosty/spadki** - zobacz jak zmienia siƒô popularno≈õƒá produkt√≥w i kategorii

### Przyk≈Çadowe u≈ºycie

```bash
# Sprawd≈∫ zmiany produktu w czasie
GET /api/products/{product_id}/changes

# Por√≥wnaj trendy kategorii
GET /api/products/categories/comparison?category=Agency

# Sprawd≈∫ metryki scrapera
GET /api/metrics/stats
```

### Synchronizacja istniejƒÖcych danych

Je≈õli masz ju≈º produkty w tabeli `products`, mo≈ºesz zsynchronizowaƒá je do `product_history`:

```bash
python scripts/sync_existing_to_history.py
```

Ten skrypt:
- ≈Åaduje wszystkie produkty z tabeli `products`
- Wstawia je do `product_history` z aktualnym timestampem
- Pomija duplikaty (na podstawie `product_id` i `scraped_at`)

