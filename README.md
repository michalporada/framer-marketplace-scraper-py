# Scraper Framer Marketplace V2

Zaawansowany scraper do zbierania danych z Framer Marketplace, umoÅ¼liwiajÄ…cy automatyzacjÄ™ pobierania informacji o:

- **Produktach**: Szablony (templates), Komponenty (components), Wektory (vectors), **Wtyczki (plugins)** â­
- **TwÃ³rcach/UÅ¼ytkownikach**: Profile z username (moÅ¼e zawieraÄ‡ znaki specjalne)
- **Kategoriach**: Kategorie produktÃ³w w marketplace
- **Recenzjach**: Opinie i oceny produktÃ³w

## ğŸ“š Dokumentacja

### GÅ‚Ã³wne Dokumenty

1. **[STACK_TECHNICZNY.md](./cursor%20documentation%20and%20rules/STACK_TECHNICZNY.md)** - SzczegÃ³Å‚owy opis stacku technicznego, w tym:
   - Biblioteki Python i narzÄ™dzia
   - Opcje baz danych
   - GitHub Actions i Vercel
   - Rekomendacje deployment

2. **[PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)** - Propozycja struktury projektu:
   - Struktura folderÃ³w
   - Opis komponentÃ³w
   - Flow scrapowania
   - Deployment strategy

3. **[REKOMENDACJE_SCRAPERA_FRAMER.md](./cursor%20documentation%20and%20rules/REKOMENDACJE_SCRAPERA_FRAMER.md)** - SzczegÃ³Å‚owa analiza Framer Marketplace:
   - Analiza techniczna strony
   - Struktura URL-i i selektory CSS
   - Zalecane dane do zbierania
   - Uwagi techniczne

## ğŸš€ Quick Start

### 1. Instalacja

```bash
# Sklonuj repozytorium
git clone <repo-url>
cd scraper-v2

# UtwÃ³rz Å›rodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt
```

### 2. Konfiguracja

```bash
# Skopiuj przykÅ‚adowy plik .env
cp .env.example .env

# Edytuj .env z odpowiednimi wartoÅ›ciami
# WiÄ™kszoÅ›Ä‡ wartoÅ›ci ma sensowne domyÅ›lne ustawienia
```

GÅ‚Ã³wne zmienne Å›rodowiskowe:
- `FRAMER_BASE_URL` - URL do Framer (domyÅ›lnie: https://www.framer.com)
- `RATE_LIMIT` - Limit requestÃ³w na sekundÄ™ (domyÅ›lnie: 1.0)
- `MAX_RETRIES` - Maksymalna liczba ponownych prÃ³b (domyÅ›lnie: 3)
- `LOG_LEVEL` - Poziom logowania (INFO, DEBUG, WARNING, ERROR)
- `CHECKPOINT_ENABLED` - WÅ‚Ä…cz checkpoint system (domyÅ›lnie: true)
- `SCRAPE_TEMPLATES`, `SCRAPE_COMPONENTS`, `SCRAPE_VECTORS`, `SCRAPE_PLUGINS` - Typy produktÃ³w do scrapowania

### 3. Uruchomienie

```bash
# Podstawowe uruchomienie (scrapuje wszystkie produkty)
python src/main.py

# Ograniczenie liczby produktÃ³w (np. 10 dla testÃ³w)
python src/main.py 10

# Export danych do CSV
python scripts/export_data.py

# Export tylko okreÅ›lonego typu produktu
python scripts/export_data.py --type template

# Setup bazy danych (opcjonalnie)
python scripts/setup_db.py --db-type postgresql
```

### 4. GitHub Actions (Automatyzacja)

Scraper moÅ¼e byÄ‡ uruchamiany automatycznie przez GitHub Actions:

- **Scheduled**: Codziennie o 2:00 UTC (zobacz `.github/workflows/scrape.yml`)
- **Manual**: RÄ™czne uruchomienie przez `workflow_dispatch`

Dane sÄ… automatycznie zapisywane jako artifacts w GitHub Actions.

## ğŸ› ï¸ Stack Techniczny

### Backend
- **Python 3.11+** - jÄ™zyk gÅ‚Ã³wny
- **httpx** - async HTTP client
- **BeautifulSoup4** - parsowanie HTML
- **pydantic v2** - walidacja danych z normalizacjÄ… (Opcja B)
- **pandas** - manipulacja danych i eksport do CSV
- **SQLAlchemy** - ORM (opcjonalnie, dla PostgreSQL)

### NarzÄ™dzia
- **structlog** - strukturalne logowanie
- **tenacity** - retry logic z exponential backoff
- **fake-useragent** - rotacja User-Agent headers
- **tqdm** - progress bars

### Deployment & Automation
- **GitHub Actions** - automatyczne scrapowanie (scheduled)
- **Vercel** - API i dashboard (opcjonalnie, Faza 3)
- **PostgreSQL/Supabase** - baza danych (opcjonalnie)

### Storage
- **JSON/CSV** - podstawowe (zalecane na start)
- **PostgreSQL** - dla wiÄ™kszych projektÃ³w
- **GitHub Artifacts** - backup danych

## ğŸ“‹ FunkcjonalnoÅ›ci

### âœ… Zaimplementowane

- [x] Scrapowanie produktÃ³w z sitemap.xml (templates/components/vectors/**plugins**)
- [x] Scrapowanie danych twÃ³rcÃ³w (profile z `@username`)
- [x] Scrapowanie kategorii (opcjonalnie)
- [x] Parsowanie recenzji produktÃ³w
- [x] Rate limiting i error handling
- [x] Zapis do JSON/CSV (organizacja wedÅ‚ug typu produktu)
- [x] Automatyzacja przez GitHub Actions
- [x] Resume capability (wznowienie po przerwie) - checkpoint system
- [x] Walidacja danych (Pydantic)
- [x] Monitoring i logowanie (structlog)
- [x] Normalizacja danych (Opcja B - raw + normalized)
- [x] ObsÅ‚uga rÃ³Å¼nych typÃ³w produktÃ³w (rÃ³Å¼ne statystyki i pola)

### ğŸ”® Opcjonalne (Faza 2+)

- [ ] API endpoints (FastAPI)
- [ ] Dashboard (Next.js)
- [ ] Baza danych (PostgreSQL) - setup script gotowy
- [ ] Error tracking (Sentry)
- [ ] Notyfikacje (Slack/Email)

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/          # Scrapery (sitemap, product, creator, category)
â”‚   â”œâ”€â”€ parsers/           # Parsery HTML (product, creator, review, category)
â”‚   â”œâ”€â”€ models/            # Modele Pydantic (Product, Creator, Review, Category)
â”‚   â”œâ”€â”€ storage/           # Zapis danych (file_storage, database)
â”‚   â”œâ”€â”€ utils/             # NarzÄ™dzia (logger, rate_limiter, retry, normalizers, checkpoint)
â”‚   â”œâ”€â”€ config/            # Konfiguracja (settings)
â”‚   â””â”€â”€ main.py            # Entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/          # Zapisane produkty (templates/, components/, vectors/, plugins/)
â”‚   â”œâ”€â”€ creators/          # Dane twÃ³rcÃ³w
â”‚   â”œâ”€â”€ categories/        # Dane kategorii
â”‚   â”œâ”€â”€ exports/           # Eksporty CSV
â”‚   â””â”€â”€ checkpoint.json    # Checkpoint dla resume capability
â”œâ”€â”€ tests/                 # Testy jednostkowe
â”œâ”€â”€ scripts/               # Skrypty pomocnicze
â”‚   â”œâ”€â”€ export_data.py     # Export do CSV
â”‚   â””â”€â”€ setup_db.py        # Setup bazy danych
â”œâ”€â”€ .github/workflows/     # GitHub Actions
â”‚   â”œâ”€â”€ scrape.yml         # Scheduled scraping
â”‚   â””â”€â”€ ci.yml             # CI/CD
â””â”€â”€ logs/                  # Logi scrapera
```

SzczegÃ³Å‚owa struktura: [PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)

## ğŸ¯ Kluczowe FunkcjonalnoÅ›ci

### Normalizacja Danych (Opcja B)
Scraper zapisuje zarÃ³wno formaty surowe z HTML jak i znormalizowane:
- **Daty**: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
- **Statystyki**: `{"raw": "19.8K Views", "normalized": 19800}`

Zapewnia to elastycznoÅ›Ä‡ w analizie i moÅ¼liwoÅ›Ä‡ weryfikacji danych ÅºrÃ³dÅ‚owych.

### Checkpoint System
Scraper automatycznie zapisuje postÄ™p scrapowania, umoÅ¼liwiajÄ…c wznowienie po przerwie:
- Automatyczne pomijanie juÅ¼ przetworzonych URL-i
- Åšledzenie nieudanych URL-i do ponownego przetworzenia
- Zapisywanie statystyk w checkpointie

### ObsÅ‚uga RÃ³Å¼nych TypÃ³w ProduktÃ³w
KaÅ¼dy typ produktu ma unikalne pola i statystyki:
- **Templates**: Pages + Views
- **Plugins**: Version + Users + Changelog
- **Components**: Installs
- **Vectors**: Users + Views + Vectors (count)

## ğŸ“Š PrzykÅ‚adowe Komendy

```bash
# Scrapowanie z limitem (test)
python src/main.py 10

# Export wszystkich produktÃ³w do CSV
python scripts/export_data.py -o data/exports/all_products.csv

# Export tylko templates
python scripts/export_data.py --type template -o data/exports/templates.csv

# Export z limitem
python scripts/export_data.py --limit 100 -o data/exports/sample.csv

# Setup PostgreSQL database
python scripts/setup_db.py --db-type postgresql

# WymuÅ› nowe scrapowanie (wyczyÅ›Ä‡ checkpoint)
rm data/checkpoint.json
python src/main.py
```

## ğŸ” Uwagi Prawne

âš ï¸ **WaÅ¼ne:**
- Przeczytaj Terms of Service Framer przed scrapowaniem
- Respektuj robots.txt
- Nie przeciÄ…Å¼aj serwerÃ³w (rate limiting)
- Nie scrapuj danych osobowych bez zgody
- RozwaÅ¼ kontakt z Framer - mogÄ… oferowaÄ‡ API

## ğŸ“ NastÄ™pne Kroki

1. **Przetestuj lokalnie** - uruchom z limitem 10-20 produktÃ³w
2. **SprawdÅº dane** - zweryfikuj jakoÅ›Ä‡ zebranych danych
3. **Skonfiguruj GitHub Actions** - dodaj secrets jeÅ›li potrzebne
4. **Rozszerz funkcjonalnoÅ›ci** - dodaj testy, monitoring, itp.

## ğŸ¤ Contributing

Projekt jest w fazie rozwoju. Wszelkie sugestie i PR-y sÄ… mile widziane!

## ğŸ“„ License

[TODO: Dodaj licencjÄ™]

---

*Ostatnia aktualizacja: 2024-12-19*

