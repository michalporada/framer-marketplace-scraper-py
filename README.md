# Scraper Framer Marketplace V2

Zaawansowany scraper do zbierania danych z Framer Marketplace, umoÅ¼liwiajÄ…cy automatyzacjÄ™ pobierania informacji o produktach, twÃ³rcach i kategoriach.

## ğŸš€ Quick Start

### 1. Instalacja

```bash
# Sklonuj repozytorium
git clone <repo-url>
cd scraper-v2

# UtwÃ³rz Å›rodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install -e .  # Lub: pip install -r requirements.txt
```

### 2. Konfiguracja

```bash
# Skopiuj przykÅ‚adowy plik .env
cp .env.example .env

# Edytuj .env z odpowiednimi wartoÅ›ciami
# WiÄ™kszoÅ›Ä‡ wartoÅ›ci ma sensowne domyÅ›lne ustawienia
```

GÅ‚Ã³wne zmienne Å›rodowiskowe:
- `FRAMER_BASE_URL` - URL do Framer (domyÅ›lnie: https://www.framer.com)
- `RATE_LIMIT` - Limit requestÃ³w na sekundÄ™ (domyÅ›lnie: 2.0)
- `MAX_RETRIES` - Maksymalna liczba ponownych prÃ³b (domyÅ›lnie: 3)
- `LOG_LEVEL` - Poziom logowania (INFO, DEBUG, WARNING, ERROR)
- `CHECKPOINT_ENABLED` - WÅ‚Ä…cz checkpoint system (domyÅ›lnie: true)
- `SCRAPE_TEMPLATES`, `SCRAPE_COMPONENTS`, `SCRAPE_VECTORS`, `SCRAPE_PLUGINS` - Typy produktÃ³w do scrapowania

### 3. Uruchomienie

```bash
# Podstawowe uruchomienie (scrapuje wszystkie produkty)
python3 -m src.main

# Ograniczenie liczby produktÃ³w (np. 10 dla testÃ³w)
python3 -m src.main 10

# Scrapowanie tylko okreÅ›lonych typÃ³w produktÃ³w
python3 -m src.main --templates-only 10    # Tylko szablony
python3 -m src.main --components-only 10     # Tylko komponenty
python3 -m src.main --vectors-only 10       # Tylko wektory
python3 -m src.main --plugins-only 10        # Tylko wtyczki

# Scrapowanie tylko kreatorÃ³w
python3 -m src.main --creators-only          # Wszyscy kreatorzy
python3 -m src.main --creators-only 10      # Z limitem
python3 -m src.main -c 10                    # KrÃ³tka wersja

# Scrapowanie tylko kategorii
python3 -m src.main --categories-only        # Wszystkie kategorie
python3 -m src.main --categories-only 10    # Z limitem
python3 -m src.main -cat 10                  # KrÃ³tka wersja
```

## ğŸ“ Struktura Projektu

```
scraper-v2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/          # Scrapery (sitemap, product, creator, category)
â”‚   â”œâ”€â”€ parsers/           # Parsery HTML (product, creator, category)
â”‚   â”œâ”€â”€ models/            # Modele Pydantic (Product, Creator, Category)
â”‚   â”œâ”€â”€ storage/           # Zapis danych (file_storage, database)
â”‚   â”œâ”€â”€ utils/             # NarzÄ™dzia (logger, rate_limiter, retry, normalizers, checkpoint)
â”‚   â”œâ”€â”€ config/            # Konfiguracja (settings)
â”‚   â””â”€â”€ main.py            # Entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/          # Zapisane produkty (templates/, components/, vectors/, plugins/)
â”‚   â”œâ”€â”€ creators/          # Profile twÃ³rcÃ³w jako osobne pliki JSON ({username}.json)
â”‚   â”œâ”€â”€ categories/       # Dane kategorii
â”‚   â”œâ”€â”€ exports/          # Eksporty CSV
â”‚   â””â”€â”€ checkpoint.json    # Checkpoint dla resume capability
â”œâ”€â”€ tests/                # Testy jednostkowe
â”œâ”€â”€ scripts/              # Skrypty pomocnicze
â”‚   â”œâ”€â”€ export_data.py    # Export do CSV
â”‚   â””â”€â”€ setup_db.py      # Setup bazy danych
â”œâ”€â”€ logs/                 # Logi scrapera
â””â”€â”€ .github/workflows/    # GitHub Actions
```

## ğŸ› ï¸ Stack Techniczny

### Backend
- **Python 3.11+** - jÄ™zyk gÅ‚Ã³wny
- **httpx** - async HTTP client
- **BeautifulSoup4** - parsowanie HTML
- **pydantic v2** - walidacja danych z normalizacjÄ…
- **pandas** - manipulacja danych i eksport do CSV
- **SQLAlchemy** - ORM (dla PostgreSQL)

### NarzÄ™dzia
- **structlog** - strukturalne logowanie
- **tenacity** - retry logic z exponential backoff
- **fake-useragent** - rotacja User-Agent headers
- **tqdm** - progress bars

### Deployment & Automation
- **GitHub Actions** - automatyczne scrapowanie (scheduled)
- **PostgreSQL/Supabase** - baza danych

### Storage
- **JSON/CSV** - podstawowe (zalecane na start)
- **PostgreSQL** - dla wiÄ™kszych projektÃ³w
- **GitHub Artifacts** - backup danych

## ğŸ“‹ FunkcjonalnoÅ›ci

### âœ… Zaimplementowane

- [x] Scrapowanie produktÃ³w z sitemap.xml (templates/components/vectors/plugins)
- [x] Scrapowanie danych twÃ³rcÃ³w (profile z `@username`)
- [x] Zapisywanie profilÃ³w twÃ³rcÃ³w jako osobne pliki JSON
- [x] Scrapowanie kategorii
- [x] Pozycja w kategorii - zbieranie pozycji szablonu w kaÅ¼dej kategorii
- [x] Rate limiting i error handling
- [x] Zapis do JSON/CSV (organizacja wedÅ‚ug typu produktu)
- [x] Eksport kreatorÃ³w do CSV
- [x] Automatyzacja przez GitHub Actions
- [x] Resume capability (wznowienie po przerwie) - checkpoint system
- [x] Walidacja danych (Pydantic)
- [x] Monitoring i logowanie (structlog)
- [x] Normalizacja danych (raw + normalized)
- [x] ObsÅ‚uga rÃ³Å¼nych typÃ³w produktÃ³w (rÃ³Å¼ne statystyki i pola)

### ğŸ”® PrzyszÅ‚e rozszerzenia

- [ ] API endpoints (FastAPI)
- [ ] Dashboard (Next.js)
- [ ] Baza danych (PostgreSQL) - setup script gotowy
- [ ] Error tracking (Sentry)
- [ ] Notyfikacje (Slack/Email)

## ğŸ¯ Kluczowe FunkcjonalnoÅ›ci

### Normalizacja Danych
Scraper zapisuje zarÃ³wno formaty surowe z HTML jak i znormalizowane:
- **Daty**: `{"raw": "5 months ago", "normalized": "2024-10-15T00:00:00Z"}`
- **Statystyki**: `{"raw": "19.8K Views", "normalized": 19800}`

### Checkpoint System
Scraper automatycznie zapisuje postÄ™p scrapowania:
- **Zawsze aktualizuje wszystkie produkty** - aby Å›ledziÄ‡ zmiany w views, cenach, statystykach
- **Åšledzenie nieudanych URL-i** - automatycznie ponawia prÃ³bÄ™ na koÅ„cu scrapowania
- **Retry failed URLs** - na koÅ„cu kaÅ¼dego scrapowania prÃ³buje ponownie przetworzyÄ‡ nieudane URL-e

## ğŸ“Š PrzykÅ‚adowe Komendy

### Export danych

```bash
# Export wszystkich produktÃ³w do CSV
python scripts/export_data.py -o data/exports/all_products.csv

# Export tylko templates
python scripts/export_data.py --type template -o data/exports/templates.csv

# Export kreatorÃ³w do CSV
python -c "from src.storage.file_storage import FileStorage; storage = FileStorage(); storage.export_creators_to_csv()"
```

### GitHub Actions

Scraper moÅ¼e byÄ‡ uruchamiany automatycznie przez GitHub Actions:
- **Scheduled**: Codziennie o 2:00 UTC
- **Manual**: RÄ™czne uruchomienie przez `workflow_dispatch`

Dane sÄ… automatycznie zapisywane jako artifacts w GitHub Actions.

## ğŸ“š Dokumentacja

SzczegÃ³Å‚owa dokumentacja znajduje siÄ™ w katalogu `cursor documentation and rules/`:

- **[README.md](./cursor%20documentation%20and%20rules/README.md)** - SzczegÃ³Å‚owa dokumentacja
- **[STACK_TECHNICZNY.md](./cursor%20documentation%20and%20rules/STACK_TECHNICZNY.md)** - Stack techniczny
- **[PROPOZYCJA_ARCHITEKTURY.md](./cursor%20documentation%20and%20rules/PROPOZYCJA_ARCHITEKTURY.md)** - Architektura projektu
- **[REKOMENDACJE_SCRAPERA_FRAMER.md](./cursor%20documentation%20and%20rules/REKOMENDACJE_SCRAPERA_FRAMER.md)** - Rekomendacje scrapowania
- **[TESTING_AND_FIXTURES.md](./cursor%20documentation%20and%20rules/TESTING_AND_FIXTURES.md)** - Dokumentacja testÃ³w

## ğŸ” Uwagi Prawne

âš ï¸ **WaÅ¼ne:**
- Przeczytaj Terms of Service Framer przed scrapowaniem
- Respektuj robots.txt
- Nie przeciÄ…Å¼aj serwerÃ³w (rate limiting)
- Nie scrapuj danych osobowych bez zgody
- RozwaÅ¼ kontakt z Framer - mogÄ… oferowaÄ‡ API

## ğŸ¤ Contributing

Projekt jest w fazie rozwoju. Wszelkie sugestie i PR-y sÄ… mile widziane!

## ğŸ“„ License

[TODO: Dodaj licencjÄ™]

---

*Ostatnia aktualizacja: 2025-01-XX*

